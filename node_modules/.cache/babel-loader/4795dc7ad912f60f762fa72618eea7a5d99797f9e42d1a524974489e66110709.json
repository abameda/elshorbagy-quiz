{"ast":null,"code":"const quizData={'Chapter 1: Introduction':[{question:\"What is the main distinction between computer architecture and computer organization?\",options:[\"Architecture deals with hardware, organization deals with software\",\"Architecture refers to attributes visible to programmer, organization refers to how features are implemented\",\"Architecture is about performance, organization is about cost\",\"There is no distinction between them\"],correct:1,explanation:\"Architecture includes instruction set, data representation, I/O mechanisms - what's visible to programmers. Organization covers control signals, interfaces, memory technology - the implementation details.\"},{question:\"Which of the following is NOT one of the four basic computer functions?\",options:[\"Data processing\",\"Data storage\",\"Data compilation\",\"Data movement\"],correct:2,explanation:\"The four basic functions are: data processing, data storage, data movement, and control. Data compilation is a software process, not a basic computer function.\"},{question:\"What does the Program Counter (PC) register contain?\",options:[\"The current instruction being executed\",\"The address of the next instruction pair to be fetched\",\"The result of the last arithmetic operation\",\"The memory address being accessed\"],correct:1,explanation:\"The Program Counter (PC) contains the address of the next instruction pair to be fetched from memory.\"},{question:\"In a multicore system, what is a 'core'?\",options:[\"The entire CPU chip\",\"An individual processing unit on a processor chip\",\"The system bus connecting components\",\"The cache memory hierarchy\"],correct:1,explanation:\"A core is an individual processing unit on a processor chip, which may be equivalent in functionality to a CPU on a single-CPU system.\"},{question:\"What is the relationship between clock frequency and clock cycle time?\",options:[\"They are the same thing\",\"Clock cycle time = clock frequency × 2\",\"Clock cycle time = 1 / clock frequency\",\"There is no mathematical relationship\"],correct:2,explanation:\"Clock cycle time is the reciprocal of clock frequency. For example, an 800 MHz clock has a cycle time of 1.25 ns.\"},{question:\"Which component manages the computer's resources and orchestrates performance?\",options:[\"ALU (Arithmetic Logic Unit)\",\"Memory\",\"Control Unit\",\"Registers\"],correct:2,explanation:\"The control unit manages the computer's resources and orchestrates the performance of its functional parts in response to instructions.\"},{question:\"What is the primary purpose of cache memory?\",options:[\"To store the operating system\",\"To backup main memory\",\"To speed up memory access by storing likely-to-be-used data\",\"To control input/output operations\"],correct:2,explanation:\"Cache memory is smaller and faster than main memory, used to speed up memory access by placing data from main memory that is likely to be used in the near future.\"},{question:\"Which register contains the 8-bit opcode instruction being executed?\",options:[\"Program Counter (PC)\",\"Instruction Register (IR)\",\"Memory Address Register (MAR)\",\"Accumulator (AC)\"],correct:1,explanation:\"The Instruction Register (IR) contains the 8-bit opcode instruction being executed.\"},{question:\"What is the function of the Memory Address Register (MAR)?\",options:[\"Contains the instruction being executed\",\"Stores temporary data for ALU operations\",\"Specifies the address in memory of the word to be written or read\",\"Controls the sequence of operations\"],correct:2,explanation:\"The Memory Address Register (MAR) specifies the address in memory of the word to be written from or read into the MBR.\"},{question:\"In embedded systems, what characterizes 'deeply embedded systems'?\",options:[\"They run complex operating systems\",\"They are programmable after deployment\",\"They are dedicated to specific tasks with extreme resource constraints\",\"They always have user interfaces\"],correct:2,explanation:\"Deeply embedded systems are dedicated, single-purpose devices with extreme resource constraints in terms of memory, processor size, time, and power consumption.\"},{question:\"What are the four main structural components of a computer?\",options:[\"CPU, RAM, ROM, Hard Drive\",\"CPU, Main Memory, I/O, System Interconnection\",\"Control Unit, ALU, Registers, Cache\",\"Processor, Memory, Storage, Network\"],correct:1,explanation:\"The four main structural components are: CPU (controls operation and data processing), Main Memory (stores data), I/O (moves data with external environment), and System Interconnection (communication mechanism).\"},{question:\"What is the difference between response time and throughput?\",options:[\"They are the same metric\",\"Response time is speed, throughput is accuracy\",\"Response time is time for one task, throughput is tasks per unit time\",\"Response time is for CPU, throughput is for memory\"],correct:2,explanation:\"Response time is the time between start and completion of a task, while throughput is the total amount of tasks done in a given time period. There is generally no relationship between these metrics.\"},{question:\"What does the Memory Buffer Register (MBR) contain?\",options:[\"The address of the next instruction\",\"A word to be stored in memory or received from memory/I/O\",\"The current instruction being decoded\",\"Control signals for the ALU\"],correct:1,explanation:\"The Memory Buffer Register (MBR) contains a word to be stored in memory or sent to the I/O unit, or is used to receive a word from memory or from the I/O unit.\"},{question:\"Which generation of IoT deployment is characterized by billions of embedded devices?\",options:[\"Information Technology (IT)\",\"Operational Technology (OT)\",\"Personal Technology\",\"Sensor/Actuator Technology\"],correct:3,explanation:\"The fourth generation, Sensor/Actuator Technology, is usually thought of as the IoT and is marked by the use of billions of embedded devices using wireless connectivity.\"},{question:\"What is the primary function of the Instruction Buffer Register (IBR)?\",options:[\"To store the program counter value\",\"To temporarily hold the right-hand instruction from memory\",\"To contain the address of data in memory\",\"To store the result of arithmetic operations\"],correct:1,explanation:\"The Instruction Buffer Register (IBR) is employed to temporarily hold the right-hand instruction from a word in memory.\"},{question:\"In the context of data movement, what distinguishes I/O from data communications?\",options:[\"Speed of data transfer\",\"Type of data being moved\",\"Distance - I/O is direct connection, data communications is over longer distances\",\"Security requirements\"],correct:2,explanation:\"I/O occurs when data are received from or delivered to a device directly connected to the computer, while data communications involves moving data over longer distances to or from remote devices.\"},{question:\"What is the key difference between application processors and dedicated processors in embedded systems?\",options:[\"Power consumption levels\",\"Manufacturing cost\",\"Application processors execute complex OS, dedicated processors serve specific tasks\",\"Physical size differences\"],correct:2,explanation:\"Application processors are defined by their ability to execute complex operating systems and are general-purpose, while dedicated processors are dedicated to one or a small number of specific tasks.\"},{question:\"Which components provide the four basic functions of integrated circuits?\",options:[\"Transistors, resistors, capacitors, inductors\",\"Gates (processing), memory cells (storage), paths (movement), control signals (control)\",\"CPU, memory, I/O, interconnection\",\"Hardware, software, firmware, middleware\"],correct:1,explanation:\"In integrated circuits: gates provide data processing, memory cells provide data storage, paths provide data movement, and control signals provide control functions.\"},{question:\"What advantage do Intel x86 and IBM System/370 families demonstrate?\",options:[\"Higher performance than competitors\",\"Lower manufacturing costs\",\"Code compatibility through shared basic architecture\",\"Better power efficiency\"],correct:2,explanation:\"All Intel x86 family and IBM System/370 family share the same basic architecture, which gives code compatibility (at least backwards), even though organization differs between versions.\"},{question:\"In deeply embedded systems, what type of processor is typically used?\",options:[\"Microprocessor\",\"Microcontroller\",\"Graphics processor\",\"Digital signal processor\"],correct:1,explanation:\"Deeply embedded systems use a microcontroller rather than a microprocessor, are not programmable once the program logic has been burned into ROM, and have no interaction with a user.\"},{question:\"What are the major structural components of a CPU?\",options:[\"Cache, registers, bus, clock\",\"Control Unit, ALU, Registers, CPU Interconnection\",\"Fetch unit, decode unit, execute unit, write-back unit\",\"L1 cache, L2 cache, L3 cache, main memory interface\"],correct:1,explanation:\"The CPU's major structural components are: Control Unit (controls CPU operation), ALU (performs data processing), Registers (provide internal storage), and CPU Interconnection (provides communication among components).\"},{question:\"What characterizes embedded systems' interaction with their environment?\",options:[\"They operate independently of external factors\",\"They are tightly coupled with real-time constraints\",\"They only process stored data\",\"They communicate only through network interfaces\"],correct:1,explanation:\"Embedded systems are often tightly coupled to their environment, giving rise to real-time constraints imposed by the need to interact with the environment, such as required speeds, precision, and timing.\"}],'Chapter 2: Storage Environment and RAID':[{question:\"What is the main difference between memory modules and storage devices?\",options:[\"Memory modules are cheaper than storage devices\",\"Memory modules use semiconductor chips while storage devices use magnetic or optical media\",\"Storage devices are faster than memory modules\",\"Memory modules are non-volatile while storage devices are volatile\"],correct:1,explanation:\"Memory modules are implemented using semiconductor chips, whereas storage devices use either magnetic or optical media. Memory modules also enable data access at higher speed than storage media.\"},{question:\"Which type of memory is volatile and requires constant power supply?\",options:[\"ROM (Read-Only Memory)\",\"RAM (Random Access Memory)\",\"Hard disk storage\",\"CD-ROM storage\"],correct:1,explanation:\"RAM is volatile and requires a constant supply of power to maintain memory cell content. Data is erased when the system's power is turned off or interrupted.\"},{question:\"What does DAS stand for and how does it connect to servers?\",options:[\"Direct Access Storage - connects via network protocols\",\"Distributed Array Storage - connects via fiber optic cables\",\"Direct Attached Storage - connects directly through Host Bus Adapter (HBA)\",\"Dynamic Allocation Storage - connects via USB only\"],correct:2,explanation:\"DAS stands for Direct Attached Storage. It connects directly to a server through Host Bus Adapter (HBA), with no network between storage and host servers.\"},{question:\"What is the main advantage of DAS over other storage environments?\",options:[\"Accessibility from multiple devices\",\"Fast performance and simple setup\",\"Network sharing capabilities\",\"Lowest cost among all options\"],correct:1,explanation:\"DAS offers fast performance, high capacity options, and simple setup as its main advantages.\"},{question:\"What is the primary disadvantage of DAS?\",options:[\"Slow performance\",\"High cost\",\"Not accessible from other devices and prone to data loss if connected computer fails\",\"Complex setup requirements\"],correct:2,explanation:\"DAS is not accessible from other devices and is prone to data loss if the connected computer fails.\"},{question:\"What does SAN stand for and what is its primary characteristic?\",options:[\"Storage Access Network - a wireless storage solution\",\"System Area Network - connects only servers\",\"Storage Area Networks - a dedicated data storage network accessible by multiple servers\",\"Secure Access Network - provides encrypted storage\"],correct:2,explanation:\"SAN stands for Storage Area Networks. It is a dedicated data storage network which can be accessed by multiple servers.\"},{question:\"Which storage environment offers the fastest performance?\",options:[\"DAS\",\"SAN\",\"NAS\",\"RAID\"],correct:1,explanation:\"SAN offers the fastest performance among the options due to its dedicated network specifically designed for high-performance connections.\"},{question:\"What are the main protocols used in SAN?\",options:[\"TCP/IP and HTTP\",\"SCSI and SATA\",\"USB and FireWire\",\"Ethernet and WiFi\"],correct:1,explanation:\"The protocols that are used in SAN are SCSI and SATA.\"},{question:\"What is the main weakness of SAN mentioned in the document?\",options:[\"Speed and Latency\",\"Security\",\"Cost\",\"Complexity\"],correct:1,explanation:\"According to the document, the weakness of SAN is Security, while Speed and Latency are listed as disadvantages.\"},{question:\"What does NAS stand for and at what level does it operate?\",options:[\"Network Access Storage - operates at block level\",\"Network Attached Storage - operates at file level\",\"Network Array Storage - operates at bit level\",\"Network Administration Storage - operates at system level\"],correct:1,explanation:\"NAS stands for Network Attached Storage. It is a file level computer data storage that connects to other devices on TCP/IP network.\"},{question:\"Which component is NOT part of NAS architecture?\",options:[\"Head unit (CPU, Memory)\",\"Network Interface Card (NIC)\",\"Host Bus Adapter (HBA)\",\"Optimized operating system\"],correct:2,explanation:\"Host Bus Adapter (HBA) is a component of DAS, not NAS. NAS components include Head unit, NIC, optimized OS, protocols, and storage protocols.\"},{question:\"What does RAID stand for?\",options:[\"Random Array of Independent Disks\",\"Redundant Array of Independent Disks\",\"Reliable Array of Integrated Disks\",\"Rapid Access of Internal Disks\"],correct:1,explanation:\"RAID stands for Redundant Array of Independent Disks.\"},{question:\"Which RAID function writes consecutive logical blocks on consecutive physical disks?\",options:[\"Mirroring\",\"Striping\",\"Parity Calculation\",\"Synchronization\"],correct:1,explanation:\"Striping writes consecutive logical byte/blocks on consecutive physical disks.\"},{question:\"Which RAID level provides striping with no parity or redundancy?\",options:[\"RAID 1\",\"RAID 0\",\"RAID 5\",\"RAID 10\"],correct:1,explanation:\"RAID 0 provides stripe with no parity, offering excellent performance but no redundancy.\"},{question:\"What is the minimum number of disks required for RAID 1?\",options:[\"1 disk\",\"2 disks\",\"3 disks\",\"4 disks\"],correct:1,explanation:\"RAID 1 requires a minimum of 2 disks and provides excellent redundancy as blocks are mirrored.\"},{question:\"Which RAID level is described as 'stripe of mirrors'?\",options:[\"RAID 0+1\",\"RAID 5\",\"RAID 10\",\"RAID 6\"],correct:2,explanation:\"RAID 10 is also called 'stripe of mirrors' and requires a minimum of 4 disks.\"},{question:\"What is the minimum number of disks required for RAID 5?\",options:[\"2 disks\",\"3 disks\",\"4 disks\",\"5 disks\"],correct:1,explanation:\"RAID 5 requires a minimum of 3 disks and provides good performance with distributed parity.\"},{question:\"Which RAID level uses byte-level striping?\",options:[\"RAID 3\",\"RAID 4\",\"RAID 5\",\"RAID 6\"],correct:0,explanation:\"RAID 3 uses byte level striping, while RAID 4 uses block level striping.\"},{question:\"What makes RAID 6 different from RAID 5?\",options:[\"It uses striping instead of mirroring\",\"It has double parity blocks and requires 6 disks\",\"It provides better performance\",\"It uses fewer disks\"],correct:1,explanation:\"RAID 6 is similar to RAID 5 except it has double parity blocks and requires 6 disks.\"},{question:\"Which RAID level is recommended for heavily read-oriented databases?\",options:[\"RAID 0\",\"RAID 1\",\"RAID 5\",\"RAID 10\"],correct:2,explanation:\"RAID 5 is the best cost effective option providing both performance and redundancy, recommended for databases that are heavily read oriented, though write operations will be slow.\"},{question:\"Which RAID level is considered the BEST option for mission critical applications?\",options:[\"RAID 0\",\"RAID 5\",\"RAID 6\",\"RAID 10\"],correct:3,explanation:\"RAID 10 provides excellent redundancy and excellent performance. If you can afford it, this is the BEST option for any mission critical applications, especially databases.\"},{question:\"Which storage environment would be best for a single user storing personal files?\",options:[\"DAS\",\"SAN\",\"NAS\",\"RAID\"],correct:0,explanation:\"DAS is best for individual workstations and storing personal data due to its fast performance and simple setup.\"},{question:\"Which storage environment is most suitable for small businesses with multiple employees sharing files?\",options:[\"DAS\",\"SAN\",\"NAS\",\"RAID\"],correct:2,explanation:\"NAS is best for small businesses and workgroups as it provides shared storage accessible from multiple devices on the network.\"},{question:\"Which storage environment requires the most technical expertise to manage?\",options:[\"DAS\",\"SAN\",\"NAS\",\"RAID\"],correct:1,explanation:\"SAN requires significant technical expertise to manage due to its specialized hardware and complex configuration.\"},{question:\"In terms of cost ranking from lowest to highest, what is the correct order?\",options:[\"SAN, NAS, DAS\",\"DAS, NAS, SAN\",\"NAS, DAS, SAN\",\"DAS, SAN, NAS\"],correct:1,explanation:\"DAS is generally the cheapest, followed by NAS, then SAN is the most expensive due to specialized hardware and configuration.\"},{question:\"Which statement about RAID is TRUE?\",options:[\"RAID guarantees complete data protection against any failure\",\"All RAID levels offer the same level of performance and redundancy\",\"RAID can be implemented on both DAS and NAS systems\",\"Using RAID eliminates the need for backups\"],correct:2,explanation:\"RAID can be implemented on both DAS and NAS systems for added benefits. RAID does not guarantee complete protection, levels differ in performance/redundancy, and backups are still needed.\"},{question:\"What type of data access does SAN provide?\",options:[\"File level data access\",\"Block level data storage\",\"Byte level data access\",\"Application level data access\"],correct:1,explanation:\"SAN provides block level data storage, allowing multiple clients to access files at the same time with very high performance.\"},{question:\"Which benefit is NOT associated with NAS?\",options:[\"Relatively inexpensive\",\"24/7 and remote data availability\",\"Fastest performance among storage options\",\"Automatic backups to other devices and cloud\"],correct:2,explanation:\"NAS has slower performance compared to DAS due to network overhead. The fastest performance is provided by SAN.\"},{question:\"What happens to RAM data when power is interrupted?\",options:[\"Data is automatically backed up\",\"Data remains intact\",\"Data is erased\",\"Data is transferred to ROM\"],correct:2,explanation:\"RAM is volatile and requires constant power supply. Data is erased when the system's power is turned off or interrupted.\"},{question:\"Which RAID level should NOT be used for critical systems?\",options:[\"RAID 0\",\"RAID 1\",\"RAID 5\",\"RAID 10\"],correct:0,explanation:\"RAID 0 has no redundancy (no mirror, no parity), so it should not be used for any critical system despite its excellent performance.\"}],'Chapter 3: Computer Architecture Introduction and Basics':[{question:\"What is the main purpose of computing according to Richard Hamming?\",options:[\"To generate numbers and data\",\"To provide insight, not just numbers\",\"To execute instructions efficiently\",\"To solve mathematical equations\"],correct:1,explanation:\"According to Richard Hamming, 'The purpose of computing is insight, not numbers.' The true value of computing lies in using data to gain understanding and knowledge (insight).\"},{question:\"Which transformation level is directly above Logic in the hierarchy?\",options:[\"Circuits\",\"Microarchitecture\",\"ISA (Architecture)\",\"Runtime System\"],correct:1,explanation:\"In the levels of transformation hierarchy, Microarchitecture is directly above Logic, which is above Circuits.\"},{question:\"What is abstraction in the context of computer systems?\",options:[\"A method to hide implementation details from users\",\"A higher level only needs to know the interface to the lower level, not how it's implemented\",\"A way to make programs run faster\",\"A technique for optimizing hardware design\"],correct:1,explanation:\"Abstraction means a higher level only needs to know about the interface to the lower level, not how the lower level is implemented.\"},{question:\"Why might you need to understand what happens in underlying abstraction levels?\",options:[\"Only for academic purposes\",\"When programs run slow, consume too much energy, or don't run correctly\",\"To impress colleagues with technical knowledge\",\"It's never necessary in modern computing\"],correct:1,explanation:\"You need to understand underlying levels when programs run slow, consume too much energy, don't run correctly, or when designing more efficient systems.\"},{question:\"What are the two key goals of the computer architecture course mentioned?\",options:[\"Learn programming and hardware design\",\"Understand processor internals and make cross-layer optimization decisions\",\"Master assembly language and digital logic\",\"Study algorithms and data structures\"],correct:1,explanation:\"The two key goals are: understand how a processor works underneath the software layer and how hardware decisions affect software/programmers, and enable making design decisions that cross boundaries of different layers.\"},{question:\"In the multi-core system slowdown example, what causes the disparity in performance between applications?\",options:[\"CPU scheduling policies\",\"Cache miss rates\",\"DRAM controller scheduling unfairness\",\"Network latency\"],correct:2,explanation:\"The disparity is caused by DRAM controller scheduling policies that are unfair to some applications, particularly the row-hit first and oldest-first policies.\"},{question:\"What does a row-conflict memory access result in compared to a row-hit access?\",options:[\"Faster access time\",\"Same access time\",\"Significantly longer access time\",\"Slightly longer access time\"],correct:2,explanation:\"A row-conflict memory access takes significantly longer than a row-hit access.\"},{question:\"What does FR-FCFS stand for in DRAM controller scheduling?\",options:[\"Fast-Ready, First-Come-First-Service\",\"First-Ready, First-Come-First-Service\",\"First-Row, First-Column-First-Service\",\"Fast-Row, First-Cache-First-Service\"],correct:1,explanation:\"FR-FCFS stands for First-Ready, First-Come-First-Service, which prioritizes row-hit accesses first, then older accesses first.\"},{question:\"Which application type does the row-hit first policy unfairly prioritize?\",options:[\"CPU-intensive applications\",\"Applications with high row buffer locality\",\"Applications with random memory access\",\"Applications with low memory usage\"],correct:1,explanation:\"Row-hit first policy unfairly prioritizes applications with high row buffer locality - threads that keep accessing the same row.\"},{question:\"What is the difference between STREAM and RANDOM memory access patterns?\",options:[\"STREAM is faster, RANDOM is slower\",\"STREAM has sequential access with high row buffer locality, RANDOM has random access with low row buffer locality\",\"STREAM uses more memory, RANDOM uses less\",\"STREAM is for reading, RANDOM is for writing\"],correct:1,explanation:\"STREAM has sequential memory access with very high row buffer locality (96% hit rate), while RANDOM has random memory access with very low row buffer locality (3% hit rate).\"},{question:\"What does a DRAM cell consist of?\",options:[\"Two transistors and a resistor\",\"A capacitor and an access transistor\",\"A flip-flop circuit\",\"Multiple logic gates\"],correct:1,explanation:\"A DRAM cell consists of a capacitor and an access transistor. It stores data in terms of charge in the capacitor.\"},{question:\"Why does DRAM need to be refreshed periodically?\",options:[\"To prevent data corruption from electromagnetic interference\",\"Because the capacitor charge leaks over time\",\"To maintain synchronization with the CPU clock\",\"To update the stored data\"],correct:1,explanation:\"DRAM capacitor charge leaks over time, so the memory controller needs to refresh each row periodically to restore charge.\"},{question:\"What is the typical refresh period for DRAM?\",options:[\"64 microseconds\",\"64 milliseconds\",\"64 seconds\",\"64 nanoseconds\"],correct:1,explanation:\"The typical refresh period is 64 ms - each row needs to be activated every 64 milliseconds.\"},{question:\"What are the downsides of DRAM refresh mentioned in the document?\",options:[\"Only energy consumption\",\"Energy consumption, performance degradation, QoS impact, and capacity scaling limits\",\"Only performance degradation\",\"Only capacity limitations\"],correct:1,explanation:\"DRAM refresh downsides include: energy consumption, performance degradation (DRAM unavailable during refresh), QoS/predictability impact (pause times), and refresh rate limits DRAM capacity scaling.\"},{question:\"According to the RAIDR research, what percentage of refresh reduction was achieved?\",options:[\"46.8%\",\"74.6%\",\"15.2%\",\"84.3%\"],correct:1,explanation:\"RAIDR achieved 74.6% refresh reduction with only 1.25KB storage overhead for 32GB memory.\"},{question:\"What is the key observation behind the RAIDR approach?\",options:[\"All DRAM rows need frequent refresh\",\"Most DRAM rows can be refreshed much less often without losing data\",\"DRAM refresh is unnecessary\",\"Refresh frequency should be increased\"],correct:1,explanation:\"RAIDR's key observation is that most DRAM rows can be refreshed much less often without losing data, allowing for different refresh rates for different rows.\"},{question:\"What are the three main steps in the RAIDR approach?\",options:[\"Reading, Writing, Refreshing\",\"Profiling, Binning, Refreshing\",\"Measuring, Sorting, Optimizing\",\"Testing, Grouping, Scheduling\"],correct:1,explanation:\"RAIDR uses three steps: 1) Profiling retention time of all rows, 2) Binning rows by retention time using Bloom Filters, 3) Refreshing rows in different bins at different rates.\"},{question:\"How much storage overhead does RAIDR require for 32GB memory?\",options:[\"1.25MB\",\"1.25KB\",\"12.5KB\",\"125KB\"],correct:1,explanation:\"RAIDR requires only 1.25KB storage for 32GB memory using efficient storage with Bloom Filters.\"},{question:\"What performance improvement did RAIDR achieve?\",options:[\"~5%\",\"~9%\",\"~16%\",\"~20%\"],correct:1,explanation:\"RAIDR achieved approximately 9% performance improvement along with 16%/20% DRAM dynamic/idle power reduction.\"},{question:\"In the memory hog example, what happens when T0 (STREAM) and T1 (RANDOM) compete for memory access?\",options:[\"They get equal access\",\"T1 gets prioritized due to oldest-first policy\",\"128 requests of T0 are serviced before T1 due to row buffer locality\",\"Random scheduling occurs\"],correct:2,explanation:\"Due to row buffer locality, 128 requests of T0 (STREAM) are serviced before T1 (RANDOM) gets access, since T0 keeps hitting the same row while T1 causes row conflicts.\"},{question:\"What is the main takeaway about abstraction layers from this chapter?\",options:[\"Abstraction layers should never be broken\",\"Breaking abstraction layers and knowing what's underneath enables problem solving and better system design\",\"Only hardware designers need to understand multiple layers\",\"Abstraction layers are only important for software development\"],correct:1,explanation:\"The main takeaway is that breaking abstraction layers and knowing what happens underneath enables you to solve problems and design better future systems.\"},{question:\"Which component in a multi-core system is shared among all cores?\",options:[\"L2 Cache\",\"L3 Cache and DRAM Memory Controller\",\"CPU registers\",\"Instruction decoder\"],correct:1,explanation:\"In the multi-core system diagram, the Shared L3 Cache and DRAM Memory Controller are shared among all cores, while each core has its own L2 cache.\"},{question:\"What makes the DRAM controller vulnerable to denial of service attacks?\",options:[\"Poor encryption mechanisms\",\"Unfair scheduling policies that can be exploited by specially written programs\",\"Insufficient bandwidth\",\"Hardware design flaws\"],correct:1,explanation:\"DRAM scheduling policies are unfair to some applications, and programs can be written to exploit this unfairness, making the controller vulnerable to denial of service attacks.\"},{question:\"What is the row size mentioned in the memory hog example?\",options:[\"4KB\",\"8KB\",\"16KB\",\"32KB\"],correct:1,explanation:\"In the memory hog example, the row size is 8KB and cache block size is 64B, resulting in 128 (8KB/64B) requests.\"},{question:\"According to the refresh overhead graphs, what percentage of performance overhead can refresh cause?\",options:[\"Up to 8%\",\"Up to 46%\",\"Up to 15%\",\"Up to 47%\"],correct:1,explanation:\"According to the refresh overhead performance graph, DRAM refresh can cause up to 46% performance overhead.\"},{question:\"What cooperation approach does the chapter suggest for solving complex system problems?\",options:[\"Hardware-only solutions\",\"Software-only solutions\",\"Cooperation between multiple components and layers\",\"Operating system level solutions only\"],correct:2,explanation:\"The chapter emphasizes that cooperation between multiple components and layers can enable more effective solutions and systems.\"},{question:\"In the levels of transformation, what is at the bottom of the hierarchy?\",options:[\"Logic\",\"Circuits\",\"Electrons\",\"Microarchitecture\"],correct:2,explanation:\"In the levels of transformation hierarchy, Electrons is at the bottom, representing the most fundamental physical level.\"},{question:\"What does the course aim to enable students to do regarding design decisions?\",options:[\"Make decisions within single layers only\",\"Focus only on software optimization\",\"Make design and optimization decisions that cross boundaries of different layers\",\"Specialize in one specific layer\"],correct:2,explanation:\"The course aims to enable students to be comfortable making design and optimization decisions that cross the boundaries of different layers and system components.\"},{question:\"What information does RAIDR expose to solve the refresh problem?\",options:[\"CPU utilization patterns\",\"Retention time profile information of DRAM rows\",\"Cache miss rates\",\"Network traffic patterns\"],correct:1,explanation:\"RAIDR exposes retention time profile information of DRAM rows to the memory controller, enabling different refresh rates for different rows based on their retention characteristics.\"},{question:\"What is the relationship between the programmer's view and hardware designer's view in computer systems?\",options:[\"They are completely independent\",\"Only the programmer's view matters\",\"The architect/microarchitect's choices critically affect both views\",\"Only the hardware designer's view is important\"],correct:2,explanation:\"The architect/microarchitect's view involves designing computers that meet system design goals, and these choices critically affect both the software programmer and the hardware designer.\"}],'Chapter 4: Introduction and Basics':[{question:\"According to Richard Hamming, what is the purpose of computing?\",options:[\"To generate numbers and data\",\"To solve mathematical equations\",\"To gain insight, not numbers\",\"To process information quickly\"],correct:2,explanation:\"Richard Hamming stated that 'The purpose of computing is insight, not numbers'. The true value of computing lies not just in generating numbers (data), but in using that data to gain understanding and knowledge (insight).\"},{question:\"What are the levels of transformation in computer systems from top to bottom?\",options:[\"Problem → Algorithm → Program → ISA → Microarchitecture → Logic → Circuits → Electrons\",\"Algorithm → Problem → Program → ISA → Logic → Microarchitecture → Circuits → Electrons\",\"Problem → Program → Algorithm → ISA → Microarchitecture → Logic → Circuits → Electrons\",\"Problem → Algorithm → ISA → Program → Microarchitecture → Logic → Circuits → Electrons\"],correct:0,explanation:\"The correct hierarchy is: Problem → Algorithm → Program/Language → Runtime System → ISA (Architecture) → Microarchitecture → Logic → Circuits → Electrons, representing the transformation from high-level problems to physical implementation.\"},{question:\"What is abstraction in the context of computer systems?\",options:[\"A method to make systems more complex\",\"A higher level only needs to know about the interface to the lower level, not how it's implemented\",\"A way to combine multiple levels into one\",\"A technique to eliminate unnecessary components\"],correct:1,explanation:\"Abstraction means that a higher level only needs to know about the interface to the lower level, not how the lower level is implemented. For example, a high-level language programmer doesn't need to know what the ISA is or how a computer executes instructions.\"},{question:\"Why might you need to understand what happens in underlying abstraction layers?\",options:[\"Only for academic purposes\",\"When programs run slow, incorrectly, or consume too much energy\",\"To make programming more difficult\",\"It's never necessary to understand underlying layers\"],correct:1,explanation:\"Understanding underlying layers becomes crucial when: the program runs slow, doesn't run correctly, consumes too much energy, or when designing more efficient and higher performance systems.\"},{question:\"In the multi-core system memory performance attack example, what causes the disparity in slowdowns between applications?\",options:[\"Different CPU speeds\",\"Cache size differences\",\"DRAM scheduling policy unfairness due to row buffer locality\",\"Operating system scheduling\"],correct:2,explanation:\"The disparity is caused by DRAM scheduling policies being unfair to some applications. Row-hit first policy unfairly prioritizes apps with high row buffer locality, while oldest-first unfairly prioritizes memory-intensive applications.\"},{question:\"What is the FR-FCFS scheduling policy in DRAM controllers?\",options:[\"First-Request, First-Come-First-Service\",\"First-Ready, First-Come-First-Service\",\"First-Row, First-Column-First-Service\",\"Fast-Response, First-Come-First-Service\"],correct:1,explanation:\"FR-FCFS stands for First-Ready, First-Come-First-Service. It has two rules: (1) Row-hit first: Service row-hit memory accesses first, (2) Oldest-first: Then service older accesses first.\"},{question:\"What makes a DRAM row-conflict access significantly slower than a row-hit access?\",options:[\"CPU processing delays\",\"Cache miss penalties\",\"Need to close current row and open new row in DRAM\",\"Network latency\"],correct:2,explanation:\"A row-conflict access requires closing the currently open row and opening a new row in DRAM, which takes significantly longer than accessing data from an already open row (row-hit).\"},{question:\"In the memory performance hog example, what characterizes the STREAM access pattern?\",options:[\"Random memory access with low row buffer locality\",\"Sequential memory access with very high row buffer locality (96% hit rate)\",\"Scattered memory access with medium locality\",\"Circular memory access pattern\"],correct:1,explanation:\"STREAM is characterized by sequential memory access with very high row buffer locality (96% hit rate) and is memory intensive, making it a memory performance hog.\"},{question:\"What characterizes the RANDOM access pattern in the memory performance example?\",options:[\"Sequential access with high locality\",\"Random memory access with very low row buffer locality (3% hit rate)\",\"Structured access with medium locality\",\"Predictable access pattern\"],correct:1,explanation:\"RANDOM is characterized by random memory access with very low row buffer locality (3% hit rate) and is similarly memory intensive compared to STREAM.\"},{question:\"What does a DRAM cell consist of?\",options:[\"Two transistors and a resistor\",\"A capacitor and an access transistor\",\"Three capacitors in series\",\"A flip-flop circuit\"],correct:1,explanation:\"A DRAM cell consists of a capacitor and an access transistor. It stores data in terms of charge in the capacitor.\"},{question:\"Why does DRAM need to be refreshed?\",options:[\"To improve performance\",\"Because capacitor charge leaks over time\",\"To reduce power consumption\",\"To increase storage capacity\"],correct:1,explanation:\"DRAM capacitor charge leaks over time, so the memory controller needs to refresh each row periodically to restore charge. Typically each row must be refreshed every 64ms.\"},{question:\"What are the main downsides of DRAM refresh?\",options:[\"Only increased cost\",\"Only performance degradation\",\"Energy consumption, performance degradation, QoS impact, and capacity scaling limits\",\"Only energy consumption\"],correct:2,explanation:\"DRAM refresh has multiple downsides: energy consumption (each refresh consumes energy), performance degradation (DRAM unavailable while refreshed), QoS/predictability impact (long pause times), and refresh rate limits DRAM capacity scaling.\"},{question:\"What is the typical refresh period for DRAM rows?\",options:[\"64 microseconds\",\"64 milliseconds\",\"64 seconds\",\"64 nanoseconds\"],correct:1,explanation:\"The typical refresh period is 64 milliseconds (64 ms). Each row must be activated (refreshed) every 64 ms to restore the charge in the capacitors.\"},{question:\"What key observation does RAIDR make about DRAM refresh?\",options:[\"All rows need frequent refresh\",\"Most DRAM rows can be refreshed much less often without losing data\",\"Refresh is unnecessary\",\"Only some rows need any refresh\"],correct:1,explanation:\"RAIDR observes that most DRAM rows can be refreshed much less often without losing data, leading to the idea of refreshing rows containing weak cells more frequently and other rows less frequently.\"},{question:\"How does RAIDR achieve refresh reduction?\",options:[\"By eliminating refresh entirely\",\"By profiling retention times, binning rows, and refreshing different bins at different rates\",\"By using different DRAM technology\",\"By increasing refresh frequency for all rows\"],correct:1,explanation:\"RAIDR works in three steps: (1) Profiling retention time of all rows, (2) Binning rows by retention time in memory controller using Bloom Filters, (3) Refreshing rows in different bins at different rates.\"},{question:\"What are the benefits achieved by RAIDR?\",options:[\"Only performance improvement\",\"74.6% refresh reduction, ~16%/20% DRAM power reduction, ~9% performance improvement\",\"Only power reduction\",\"Only refresh reduction\"],correct:1,explanation:\"RAIDR achieves multiple benefits: 74.6% refresh reduction with only 1.25KB storage overhead, ~16%/20% DRAM dynamic/idle power reduction, and ~9% performance improvement, with benefits increasing with DRAM capacity.\"},{question:\"What is one of the two key goals of the computer architecture course mentioned in the lecture?\",options:[\"To learn programming languages\",\"To understand how a processor works underneath the software layer\",\"To design operating systems\",\"To build hardware components\"],correct:1,explanation:\"One key goal is to understand how a processor works underneath the software layer and how decisions made in hardware affect the software/programmer.\"},{question:\"What is the second key goal of the computer architecture course?\",options:[\"To memorize instruction sets\",\"To enable making design and optimization decisions that cross boundaries of different layers\",\"To focus only on hardware design\",\"To specialize in one abstraction layer\"],correct:1,explanation:\"The second key goal is to enable students to be comfortable in making design and optimization decisions that cross the boundaries of different layers and system components.\"},{question:\"In the memory performance hog scenario, approximately how many requests of the STREAM application (T0) are serviced before the RANDOM application (T1) gets served?\",options:[\"64 requests\",\"96 requests\",\"128 requests\",\"256 requests\"],correct:2,explanation:\"With a row size of 8KB and cache block size of 64B, there are 128 (8KB/64B) requests of T0 (STREAM) serviced before T1 (RANDOM) gets a chance, demonstrating the unfairness of the row-hit first policy.\"},{question:\"What is the main takeaway about abstraction layers from this lecture?\",options:[\"Abstraction layers should never be crossed\",\"Breaking abstraction layers and knowing what is underneath enables problem solving\",\"Only hardware designers need to understand multiple layers\",\"Abstraction layers are only theoretical concepts\"],correct:1,explanation:\"The main takeaway is that breaking the abstraction layers (between components and transformation hierarchy levels) and knowing what is underneath enables you to solve problems and design better future systems. Cooperation between multiple components and layers can enable more effective solutions.\"}],'Chapter 5: What is A Computer and Von Neumann Model':[{question:\"What are the three key components that define a computer?\",options:[\"Hardware, software, and users\",\"Computation, communication, and storage (memory)\",\"Input, processing, and output\",\"CPU, RAM, and hard drive\"],correct:1,explanation:\"A computer is defined by three key components: Computation (processing), Communication (I/O), and Storage (memory). These components work together to form a complete computing system.\"},{question:\"What are the two key properties of the Von Neumann model?\",options:[\"Fast processing and large memory\",\"Stored program and sequential instruction processing\",\"Multiple cores and parallel processing\",\"Input/output capabilities and user interface\"],correct:1,explanation:\"The Von Neumann model has two key properties: (1) Stored program - instructions stored in linear memory array with unified memory for instructions and data, (2) Sequential instruction processing - one instruction processed at a time with Program Counter identifying current instruction.\"},{question:\"In the Von Neumann model, what determines whether a stored value is interpreted as an instruction?\",options:[\"The value itself\",\"The memory location\",\"The control signals\",\"The data type\"],correct:2,explanation:\"In the Von Neumann model, the interpretation of a stored value depends on the control signals. The same bit pattern can be interpreted as data or as an instruction depending on how the control unit processes it.\"},{question:\"What is another name for the Von Neumann architecture?\",options:[\"Parallel processing computer\",\"Stored program computer\",\"Data flow computer\",\"Multi-core computer\"],correct:1,explanation:\"The Von Neumann architecture is also called a 'stored program computer' because instructions are stored in memory along with data, rather than being hardwired into the machine.\"},{question:\"In the Von Neumann model, how is the Program Counter (instruction pointer) advanced?\",options:[\"Randomly based on available instructions\",\"Based on data availability\",\"Sequentially except for control transfer instructions\",\"In parallel for multiple instructions\"],correct:2,explanation:\"The Program Counter is advanced sequentially except for control transfer instructions (like jumps, branches, calls). This sequential advancement is a fundamental characteristic of Von Neumann execution.\"},{question:\"In the dataflow model, when is an instruction executed?\",options:[\"When the instruction pointer points to it\",\"When all its operands are ready\",\"In sequential order\",\"When the CPU is idle\"],correct:1,explanation:\"In the dataflow model, an instruction is executed when all its operands are ready (i.e., when all inputs have tokens). There is no instruction pointer - execution is driven by data availability.\"},{question:\"What is the main difference between Von Neumann and dataflow execution models?\",options:[\"Von Neumann uses more memory\",\"Von Neumann is control-driven/sequential, dataflow is data-driven/parallel\",\"Dataflow is slower than Von Neumann\",\"Von Neumann requires special hardware\"],correct:1,explanation:\"Von Neumann model is control-driven with sequential execution (instruction pointer controls order), while dataflow model is data-driven with potentially parallel execution (data availability controls order).\"},{question:\"Which execution model is inherently more parallel?\",options:[\"Von Neumann model\",\"Dataflow model\",\"Both are equally parallel\",\"Neither supports parallelism\"],correct:1,explanation:\"The dataflow model is inherently more parallel because multiple instructions can 'fire' (execute) simultaneously when their operands are ready, unlike Von Neumann's sequential execution model.\"},{question:\"In a dataflow machine, what causes a data flow node to 'fire'?\",options:[\"A clock signal\",\"The instruction pointer\",\"When all its inputs have tokens (are ready)\",\"A random trigger\"],correct:2,explanation:\"A data flow node fires (is fetched and executed) when all its inputs are ready, i.e., when all inputs have tokens. This is the fundamental execution principle of dataflow computing.\"},{question:\"What major instruction set architectures use the Von Neumann model today?\",options:[\"Only x86\",\"x86, ARM, MIPS, SPARC, Alpha, POWER\",\"Only ARM and x86\",\"Only older architectures\"],correct:1,explanation:\"All major instruction set architectures today use the Von Neumann model, including x86, ARM, MIPS, SPARC, Alpha, and POWER architectures.\"},{question:\"At the microarchitecture level, how do modern processors actually execute instructions?\",options:[\"Exactly as specified by Von Neumann model\",\"Very differently from Von Neumann model (pipelined, out-of-order, etc.)\",\"Only in sequential order\",\"Without any optimization\"],correct:1,explanation:\"Modern microarchitectures execute very differently from the Von Neumann model - using pipelined execution, multiple instructions at a time, out-of-order execution, and separate instruction/data caches, but this is not exposed to software.\"},{question:\"What is the key difference between ISA and microarchitecture?\",options:[\"ISA is hardware, microarchitecture is software\",\"ISA is the agreed interface between SW/HW, microarchitecture is the specific implementation\",\"They are the same thing\",\"ISA is old, microarchitecture is new\"],correct:1,explanation:\"ISA is the agreed upon interface between software and hardware (what software writer needs to know), while microarchitecture is the specific implementation of an ISA (not visible to software).\"},{question:\"Using the car analogy, what represents ISA vs. microarchitecture?\",options:[\"Engine vs. wheels\",\"Gas pedal (interface) vs. engine internals (implementation)\",\"Steering wheel vs. brakes\",\"Exterior vs. interior\"],correct:1,explanation:\"The gas pedal represents ISA (interface for 'acceleration' that driver uses), while the internals of the engine represent microarchitecture (how 'acceleration' is actually implemented).\"},{question:\"Which changes faster: ISA or microarchitecture?\",options:[\"ISA changes faster\",\"Microarchitecture changes faster\",\"They change at the same rate\",\"Neither changes\"],correct:1,explanation:\"Microarchitecture usually changes faster than ISA. There are few ISAs (x86, ARM, SPARC, MIPS, Alpha) but many microarchitectures. For example, x86 ISA has many implementations: 286, 386, 486, Pentium, Pentium Pro, Pentium 4, Core, etc.\"},{question:\"What does superscalar processing refer to?\",options:[\"Using multiple CPU cores\",\"A technique to execute multiple instructions in parallel within the same processor core\",\"Increasing clock frequency\",\"Adding more memory\"],correct:1,explanation:\"Superscalar processing is a technique used in modern microprocessor design to increase instruction throughput by executing multiple instructions in parallel within the same processor core, allowing more than one instruction per clock cycle.\"},{question:\"Which of the following is part of the ISA?\",options:[\"Number of ports to the register file\",\"ADD instruction's opcode\",\"Whether machine employs pipelined execution\",\"Number of cycles to execute MUL instruction\"],correct:1,explanation:\"ADD instruction's opcode is part of the ISA as it defines the instruction interface. Number of register file ports, pipelining, and execution cycles are microarchitecture implementation details not visible to software.\"},{question:\"Which of the following is part of microarchitecture?\",options:[\"Number of general purpose registers\",\"Instruction opcodes\",\"Number of ports to the register file\",\"Memory addressing modes\"],correct:2,explanation:\"Number of ports to the register file is a microarchitecture detail (implementation choice for performance). Number of registers, opcodes, and addressing modes are ISA specifications visible to programmers.\"},{question:\"What does ISA specify regarding instructions?\",options:[\"Only the instruction format\",\"Opcodes, addressing modes, data types, instruction types and formats, registers, condition codes\",\"Only the execution time\",\"Only the memory requirements\"],correct:1,explanation:\"ISA specifies comprehensive instruction-related elements: opcodes, addressing modes, data types, instruction types and formats, registers, and condition codes - everything a programmer needs to know to write programs.\"},{question:\"Which of the following are microarchitecture implementation choices?\",options:[\"Virtual memory management\",\"Pipelining, out-of-order execution, caching policies, superscalar processing\",\"Instruction set definition\",\"Memory addressing modes\"],correct:1,explanation:\"Microarchitecture includes implementation choices like pipelining, in-order vs out-of-order execution, memory access scheduling, superscalar processing, caching policies, prefetching, etc. - all done without exposure to software.\"},{question:\"In the out-of-order execution example with instructions (1) mov eax,0 (2) mov edx,1 (3) mov edx,3 (4) inc edx (5) mov ecx,3, what determines the execution order?\",options:[\"The original program order must be maintained\",\"The transistors/hardware decides based on dependencies and available resources\",\"Random selection\",\"Always execute in reverse order\"],correct:1,explanation:\"In out-of-order execution, the transistors (hardware) decide which instructions to execute based on data dependencies and available execution resources, while maintaining the correct program semantics.\"}],'Chapter 6: ISA Tradeoffs':[{question:\"What is a design point in computer architecture?\",options:[\"A specific location on the processor chip\",\"A set of design considerations and their importance that leads to tradeoffs\",\"The final stage of processor design\",\"A testing methodology for processors\"],correct:1,explanation:\"A design point is a set of design considerations and their importance that leads to tradeoffs in both ISA and microarchitecture. It's determined by the application space and intended users/market.\"},{question:\"Which of the following are key design considerations mentioned in the lecture?\",options:[\"Only cost and performance\",\"Cost, performance, power consumption, energy consumption, availability, reliability, time to market\",\"Only performance and reliability\",\"Hardware complexity and software compatibility\"],correct:1,explanation:\"The key design considerations include: Cost, Performance, Maximum power consumption, Energy consumption (battery life), Availability, Reliability and Correctness, and Time to Market.\"},{question:\"What determines the design point of a computer system?\",options:[\"The available technology\",\"The manufacturing cost\",\"The 'Problem' space (application space) and intended users/market\",\"Government regulations\"],correct:2,explanation:\"The design point is determined by the 'Problem' space (application space) and the intended users/market, which influences the relative importance of different design considerations.\"},{question:\"What are the two main components of an instruction?\",options:[\"Address and data\",\"Opcode and operands\",\"Source and destination\",\"Input and output\"],correct:1,explanation:\"An instruction consists of: (1) opcode - what the instruction does, and (2) operands - who it is to do it to. This is the basic element of the HW/SW interface.\"},{question:\"What is the concept of 'bit steering' in instruction encoding?\",options:[\"Using bits to control data flow direction\",\"A bit in the instruction determines the interpretation of other bits\",\"Steering bits toward the ALU\",\"Managing bit-level operations\"],correct:1,explanation:\"Bit steering is a concept where a bit in the instruction determines the interpretation of other bits, allowing for more efficient use of the instruction encoding space.\"},{question:\"In a 0-address (stack) machine, how are operations performed?\",options:[\"Using registers only\",\"Operations work on top elements of the stack (push/pop)\",\"Direct memory addressing\",\"Using accumulator register\"],correct:1,explanation:\"In a 0-address stack machine, operations work on the top elements of the stack. Operands are pushed onto the stack, operations are performed on stack top elements, and results are popped off.\"},{question:\"What characterizes a 1-address (accumulator) machine?\",options:[\"All operations use stack\",\"Operations use accumulator register (op ACC, ld A, st A)\",\"Two operands per instruction\",\"Three separate operands\"],correct:1,explanation:\"In a 1-address accumulator machine, operations typically involve the accumulator register (ACC). Instructions like 'op ACC', 'ld A' (load into ACC), 'st A' (store from ACC) are characteristic.\"},{question:\"In a 2-address machine, what happens to one of the operands?\",options:[\"It remains unchanged\",\"One operand is both source and destination (gets clobbered)\",\"It gets copied to memory\",\"It gets pushed to stack\"],correct:1,explanation:\"In a 2-address machine (op S,D), one operand serves as both source and destination, meaning the original value gets overwritten (clobbered) with the operation result.\"},{question:\"What is the main advantage of a 3-address machine?\",options:[\"Smaller instruction size\",\"Source and destination are separate (op S1,S2,D)\",\"Faster execution\",\"Lower power consumption\"],correct:1,explanation:\"In a 3-address machine (op S1,S2,D), the source operands and destination are separate, which means source values are preserved and not clobbered during operations.\"},{question:\"What are the main advantages of stack machines?\",options:[\"Large instruction size and complex logic\",\"Small instruction size, simpler logic, compact code, efficient procedure calls\",\"High flexibility and parallel operations\",\"Complex data type support\"],correct:1,explanation:\"Stack machines have: small instruction size (no operands needed for operate instructions), simpler logic, compact code, and efficient procedure calls (all parameters on stack with no additional cycles for parameter passing).\"},{question:\"What are the main disadvantages of stack machines?\",options:[\"Large code size and slow execution\",\"Computations not easily expressible in postfix notation are difficult; limited flexibility\",\"High power consumption\",\"Complex instruction decoding\"],correct:1,explanation:\"Stack machines have disadvantages: computations not easily expressible with postfix notation are difficult to map, cannot perform operations on many values simultaneously (only top N values), and lack flexibility.\"},{question:\"The PDP-11 is an example of which type of machine?\",options:[\"0-address (stack) machine\",\"1-address (accumulator) machine\",\"2-address machine\",\"3-address machine\"],correct:2,explanation:\"The PDP-11 is a 2-address machine. Its ADD instruction has a 4-bit opcode and 2 6-bit operand specifiers, with limited bits to specify an instruction.\"},{question:\"What is the main disadvantage of the PDP-11's 2-address design?\",options:[\"Too many operands\",\"One source operand is always clobbered with the result\",\"Instructions are too long\",\"Cannot access memory\"],correct:1,explanation:\"In PDP-11's 2-address design, one source operand is always clobbered (overwritten) with the result of the instruction, requiring additional steps to preserve original values when needed.\"},{question:\"What type of machine is the Alpha architecture?\",options:[\"2-address memory/memory machine\",\"Stack machine\",\"3-address load/store machine\",\"1-address accumulator machine\"],correct:2,explanation:\"Alpha is a 3-address load/store machine, meaning it has separate source and destination operands, and memory access is only through explicit load and store instructions.\"},{question:\"What type of machine is x86?\",options:[\"3-address load/store machine\",\"2-address memory/memory machine\",\"Stack machine only\",\"1-address accumulator machine\"],correct:1,explanation:\"x86 is a 2-address memory/memory machine, meaning it can perform operations directly between memory locations and registers, with one operand serving as both source and destination.\"},{question:\"How is a data type defined in ISA context?\",options:[\"Any binary representation\",\"Representation of information for which there are instructions that operate on the representation\",\"Only primitive types like integers\",\"Memory storage format only\"],correct:1,explanation:\"A data type is defined as a representation of information for which there are instructions that operate on that representation. It's not just about storage format, but about having instruction support.\"},{question:\"Which of the following are examples of data types mentioned?\",options:[\"Only integer and floating point\",\"Integer, floating point, character, binary, decimal, BCD, doubly linked list, queue, string, bit vector, stack\",\"Only primitive data types\",\"Only numeric types\"],correct:1,explanation:\"The lecture mentions various data types: Integer, floating point, character, binary, decimal, BCD, doubly linked list, queue, string, bit vector, and stack - ranging from primitive to complex structured types.\"},{question:\"What is an example of a high-level data type instruction from VAX?\",options:[\"ADD and SUB only\",\"INSQUEUE (Insert Queue) and REMQUEUE (Remove Queue) on doubly linked lists\",\"Only load and store\",\"Basic arithmetic operations\"],correct:1,explanation:\"VAX provided high-level instructions like INSQUEUE (Insert Queue) and REMQUEUE (Remove Queue) that operated on doubly linked lists or queues, and FINDFIRST for complex data structure operations.\"},{question:\"What does the 'semantic gap' refer to in computer architecture?\",options:[\"Physical distance between components\",\"The disparity between high-level software concepts and low-level hardware operations\",\"Time delay in instruction execution\",\"Memory access latency\"],correct:1,explanation:\"The semantic gap refers to the disparity between high-level concepts and abstractions used in software programming and the low-level operations and mechanisms implemented in hardware.\"},{question:\"How do Early RISC architectures and Intel 432 differ in terms of semantic gap?\",options:[\"Both have the same approach\",\"Early RISC: only integer data type (large gap); Intel 432: object data type, capability-based (small gap)\",\"Both focus on complex data types\",\"Both use only primitive types\"],correct:1,explanation:\"Early RISC architectures had only integer data types (creating a large semantic gap), while Intel 432 supported object data types and was capability-based (attempting to close the semantic gap with high-level features).\"}],'Chapter 7: ISA Tradeoffs':[// Data Types (Pages 3-5)\n{question:\"Which ISA introduced dedicated instructions for doubly linked list operations like INSQUEUE?\",options:[\"x86\",\"VAX\",\"MIPS\",\"ARM\"],correct:1,explanation:\"VAX had specialized instructions for queue/list operations, reflecting its CISC design philosophy.\"},{question:\"What was the primary data type supported in early RISC architectures?\",options:[\"Floating point\",\"Integer only\",\"Object references\",\"Binary-coded decimal\"],correct:1,explanation:\"Early RISC designs like MIPS supported only integers to maintain simplicity.\"},// Semantic Gap (Pages 5,25)\n{question:\"The Intel 432's object data types illustrate what kind of semantic gap approach?\",options:[\"Maximized gap\",\"Minimized gap\",\"No gap\",\"Variable gap\"],correct:1,explanation:\"Intel 432 aimed to minimize the gap by supporting high-level constructs directly in hardware.\"},// Memory Organization (Pages 6-8)\n{question:\"In a 32-bit addressable system like the first Alpha, how would you add two 32-bit numbers?\",options:[\"Using two 32-bit load/store instructions\",\"With a single 64-bit ADD instruction\",\"Through memory-memory operations\",\"Using bit-addressable operations\"],correct:0,explanation:\"Required multiple operations due to 32-bit addressability constraints.\"},{question:\"Which supercomputer architecture used 64-bit addressability?\",options:[\"Burroughs 1700\",\"Cray-1\",\"Intel 432\",\"VAX-11\"],correct:1,explanation:\"Cray supercomputers pioneered 64-bit addressability for scientific computing.\"},// Endianness (Pages 7-8)\n{question:\"Which architecture uses big-endian byte ordering?\",options:[\"x86\",\"PowerPC\",\"ARM (little-endian mode)\",\"Original PCI bus\"],correct:1,explanation:\"PowerPC and SPARC are notable big-endian architectures.\"},{question:\"In little-endian systems, how is the value 0x12345678 stored at address A?\",options:[\"A:12 A+1:34 A+2:56 A+3:78\",\"A:78 A+1:56 A+2:34 A+3:12\",\"A:56 A+1:78 A+2:12 A+3:34\",\"Split across cache lines\"],correct:1,explanation:\"Little-endian stores least significant byte at lowest address.\"},// Registers (Pages 9-12)\n{question:\"What characteristic of programs justifies having registers in an ISA?\",options:[\"Spatial locality\",\"Data locality (temporal and spatial)\",\"Memory wall effect\",\"Von Neumann bottleneck\"],correct:1,explanation:\"Registers exploit temporal locality (reused data) and spatial locality (nearby data).\"},{question:\"How many general-purpose registers did IA-64 (Itanium) introduce?\",options:[\"8\",\"16\",\"32\",\"128\"],correct:3,explanation:\"IA-64 expanded to 128 registers for explicit parallelism.\"},// Programmer Invisible State (Pages 10-11)\n{question:\"Why can't programmers directly access pipeline registers?\",options:[\"They are protected by the OS\",\"They represent microarchitectural state\",\"They are physically inaccessible\",\"They violate memory protection\"],correct:1,explanation:\"Pipeline registers are part of implementation-specific microarchitecture.\"},// Instruction Classes (Page 13)\n{question:\"Which instruction class changes the sequence of execution?\",options:[\"Operate instructions\",\"Data movement instructions\",\"Control flow instructions\",\"Floating-point instructions\"],correct:2,explanation:\"Control flow instructions (branches/jumps) alter the PC.\"},// Addressing Modes (Pages 14-19)\n{question:\"Which addressing mode combines a base register and index register?\",options:[\"Displacement\",\"Register indirect\",\"Indexed addressing\",\"Memory indirect\"],correct:2,explanation:\"Indexed addressing uses base + index calculation.\"},{question:\"What does 'orthogonal ISA' mean?\",options:[\"Instructions use only right angles\",\"All instructions can use all addressing modes\",\"Fixed 90-degree instruction alignment\",\"Separate integer/floating-point pipelines\"],correct:1,explanation:\"Orthogonality means uniform combination of operations and addressing modes.\"},// Instruction Formats (Pages 20-22,28-29)\n{question:\"What is a key advantage of variable-length instructions?\",options:[\"Simpler hardware decoding\",\"Better code density\",\"Faster clock speeds\",\"More registers\"],correct:1,explanation:\"Variable-length enables compact encoding (e.g., x86).\"},{question:\"Which field in MIPS I-type instructions holds the immediate value?\",options:[\"rs\",\"rt\",\"opcode\",\"16-bit immediate field\"],correct:3,explanation:\"I-type uses 16-bit immediate for constants/offsets.\"},// Complex vs Simple Instructions (Pages 23-26)\n{question:\"What was a major disadvantage of complex CISC instructions?\",options:[\"Limited compiler optimization opportunities\",\"Too many registers\",\"Fixed-length encoding\",\"Lack of virtual memory support\"],correct:0,explanation:\"Complex instructions created coarse-grained operations that constrained optimizations.\"},{question:\"Which VAX instruction provided array access with bounds checking?\",options:[\"MOV\",\"INDEX\",\"BOUNDS\",\"ARRAY\"],correct:1,explanation:\"VAX INDEX instruction exemplified high-level language support.\"},// RISC vs CISC (Pages 26-27)\n{question:\"Which characteristic is NOT typical of RISC designs?\",options:[\"Many addressing modes\",\"Uniform decode\",\"Fixed-length instructions\",\"Load/store architecture\"],correct:0,explanation:\"RISC minimizes addressing modes for simplicity.\"},{question:\"What motivated the x86's instruction prefixes?\",options:[\"Backward compatibility\",\"Faster decoding\",\"Fewer registers\",\"Big-endian support\"],correct:0,explanation:\"Prefixes allowed extending the ISA while maintaining compatibility.\"},// Evolution (Page 27)\n{question:\"What drove ISA evolution according to the chapter?\",options:[\"Compiler limitations\",\"Memory constraints\",\"Specialization needs\",\"All of the above\"],correct:3,explanation:\"All these factors historically influenced ISA design.\"},// Instruction Formats Deep Dive (Pages 28-29)\n{question:\"How many bytes can x86 instruction prefixes occupy?\",options:[\"0\",\"1\",\"Up to 4\",\"Exactly 2\"],correct:2,explanation:\"x86 allows up to four 1-byte prefixes.\"},{question:\"What MIPS instruction format has a 26-bit immediate?\",options:[\"R-type\",\"I-type\",\"J-type\",\"U-type\"],correct:2,explanation:\"J-type (jump) uses 26-bit immediates for address targets.\"}],'Chapter 8: Single-Cycle Microarchitecture':[{question:\"What is the primary purpose of cache memory?\",options:[\"To permanently store all program instructions\",\"To store active/commonly used instructions and speed up processing\",\"To replace the main memory entirely\",\"To store only the results of arithmetic operations\"],correct:1,explanation:\"Cache memory temporarily stores active or frequently used instructions to speed up processing and reduce bottlenecks between RAM and the CPU.\"},{question:\"During the fetch part of the instruction cycle, what is the role of the address bus?\",options:[\"It carries the instruction's opcode to the ALU\",\"It carries the address of the instruction to main memory\",\"It stores the result of the executed instruction\",\"It decodes the instruction for the control unit\"],correct:1,explanation:\"The address bus carries the memory address of the instruction to be fetched from main memory to the CPU.\"},{question:\"Which of the following is a drawback of higher clock speeds in CPUs?\",options:[\"Fewer instructions can be executed per second\",\"The CPU may overheat and require more cooling\",\"Programs run slower due to increased complexity\",\"The data bus width is reduced\"],correct:1,explanation:\"Higher clock speeds increase power consumption and heat generation, potentially causing overheating and requiring additional cooling solutions.\"},{question:\"In a single-cycle microarchitecture, how many clock cycles does each instruction take to execute?\",options:[\"One cycle\",\"Two cycles\",\"Variable cycles depending on the instruction\",\"Six cycles (one per phase)\"],correct:0,explanation:\"In a single-cycle microarchitecture, all phases of an instruction (fetch, decode, execute, etc.) are completed within a single clock cycle.\"},{question:\"What determines the clock cycle time in a single-cycle microarchitecture?\",options:[\"The fastest instruction\",\"The average instruction latency\",\"The slowest instruction\",\"The number of functional units\"],correct:2,explanation:\"The slowest instruction dictates the clock cycle time because all instructions must complete within one cycle.\"}],'Chapter 9: Multi-Cycle and Pipelined Microarchitecture':[{question:\"How does a multi-cycle microarchitecture differ from a single-cycle design?\",options:[\"Instructions always take fewer cycles to complete\",\"Each phase of the instruction cycle may span multiple clock cycles\",\"It eliminates the need for a control unit\",\"It uses a smaller data bus\"],correct:1,explanation:\"In a multi-cycle microarchitecture, each phase (fetch, decode, etc.) can take multiple clock cycles, allowing for shorter cycle times.\"},{question:\"What is a key advantage of multi-cycle machines over single-cycle machines?\",options:[\"The slowest instruction determines the cycle time\",\"Clock cycle time is shorter as it depends on the slowest stage, not instruction\",\"They require no control signals\",\"All instructions execute in parallel\"],correct:1,explanation:\"Multi-cycle machines break instructions into stages, allowing the clock cycle time to be determined by the slowest stage rather than the slowest instruction.\"},{question:\"Which component generates control signals to coordinate the datapath in instruction processing?\",options:[\"ALU\",\"Cache memory\",\"Control logic\",\"Address bus\"],correct:2,explanation:\"The control logic decodes instructions and generates signals to direct the datapath (e.g., ALU, registers) on how to process data.\"},{question:\"What happens to the program counter (PC) during the fetch stage?\",options:[\"It is reset to zero\",\"It is incremented to point to the next instruction\",\"It stores the result of the ALU operation\",\"It holds the opcode for decoding\"],correct:1,explanation:\"After fetching an instruction, the PC is incremented to point to the next instruction in memory.\"},{question:\"Which of the following is true about the execute stage?\",options:[\"It retrieves the instruction from main memory\",\"It decodes the opcode into control signals\",\"It performs the actual operation (e.g., ALU computation)\",\"It stores the result in the instruction register\"],correct:2,explanation:\"The execute stage carries out the operation specified by the instruction, such as arithmetic in the ALU or data transfer.\"}],'Chapter 10: Introduction to Microarchitecture':[{question:\"What is the key characteristic of a single-cycle microarchitecture?\",options:[\"All instructions complete execution in one clock cycle\",\"Instructions are pipelined across multiple cycles\",\"Only arithmetic instructions use a single cycle\",\"Memory access takes variable cycles depending on latency\"],correct:0,explanation:\"In a single-cycle microarchitecture, every instruction (e.g., ALU, load/store) completes all stages (fetch, decode, execute, etc.) within one clock cycle.\"},{question:\"Which component is responsible for holding the current instruction address in the single-cycle datapath?\",options:[\"Register file\",\"Program counter (PC)\",\"ALU\",\"Data memory\"],correct:1,explanation:\"The program counter (PC) stores the address of the current instruction being executed and increments by 4 (for MIPS) after each fetch.\"},{question:\"What is the role of the 'sign-extend' unit in the datapath?\",options:[\"To convert 16-bit immediate values to 32-bit signed values\",\"To perform arithmetic operations\",\"To select between register or immediate operands\",\"To manage memory addresses\"],correct:0,explanation:\"The sign-extend unit expands 16-bit immediate values (e.g., in I-type instructions) to 32 bits while preserving the sign for correct arithmetic operations.\"},{question:\"Which control signal determines whether the ALU uses a register value or an immediate value as its second operand?\",options:[\"RegWrite\",\"ALUSrc\",\"MemtoReg\",\"Branch\"],correct:1,explanation:\"ALUSrc selects between the second register operand (e.g., for R-type) or a sign-extended immediate (e.g., for ADDI/LW/SW).\"},{question:\"What happens during the 'MEM' stage of the LW instruction?\",options:[\"The ALU computes the effective memory address\",\"Data is read from memory and written to a register\",\"The instruction is fetched from memory\",\"The PC is updated\"],correct:1,explanation:\"In the MEM stage, the data memory is accessed using the address computed in EX, and the result is later written back to a register in WB.\"},{question:\"Which instruction type requires the 'RegDst' control signal to select the destination register from bits [15:11]?\",options:[\"I-type (e.g., ADDI)\",\"R-type (e.g., ADD)\",\"Load (e.g., LW)\",\"Store (e.g., SW)\"],correct:1,explanation:\"R-type instructions use bits [15:11] for the destination register (rd), while I-type (e.g., ADDI/LW) use bits [20:16] (rt).\"},{question:\"Why is the single-cycle design inefficient for real-world implementations?\",options:[\"It cannot handle branch instructions\",\"The clock cycle must accommodate the slowest instruction (e.g., LW)\",\"It lacks a register file\",\"Memory accesses are asynchronous\"],correct:1,explanation:\"The clock cycle length is determined by the slowest instruction (e.g., LW, which uses memory access), making faster instructions (e.g., ADD) unnecessarily slow.\"},{question:\"What is the purpose of the 'MemtoReg' multiplexer in the datapath?\",options:[\"To select between ALU result or memory data for register writeback\",\"To choose between register operands\",\"To extend immediate values\",\"To compute branch targets\"],correct:0,explanation:\"MemtoReg selects whether the writeback data comes from the ALU result (e.g., for ADD) or memory (e.g., for LW).\"}]};// Arabic translation of quiz data\nconst quizDataArabic={};// Create a mapping between English and Arabic chapter names\nconst chapterMapping={'Chapter 1: Introduction':'الفصل 1: مقدمة','Chapter 2: Storage Environment and RAID':'الفصل 2: ','Chapter 3: Computer Architecture Introduction and Basics':'الفصل 3: ','Chapter 4: Introduction and Basics':'الفصل 4: ','Chapter 5: What is A Computer and Von Neumann Model':'الفصل 5: ','Chapter 6: Introduction to ISA Tradeoffs':'الفصل 6: ','Chapter 7: ISA Tradeoffs afterlecture':'الفصل 7: ','Chapter 8: Single-Cycle Microarchitecture':'الفصل 8: ','Chapter 9: Multi-Cycle and Pipelined Microarchitecture':'الفصل 9: ','Chapter 10: Introduction to Microarchitecture':'الفصل 10: '};// Reverse mapping (Arabic to English)\nconst reverseChapterMapping={};Object.keys(chapterMapping).forEach(englishName=>{reverseChapterMapping[chapterMapping[englishName]]=englishName;});export{quizData,quizDataArabic,chapterMapping,reverseChapterMapping};","map":{"version":3,"names":["quizData","question","options","correct","explanation","quizDataArabic","chapterMapping","reverseChapterMapping","Object","keys","forEach","englishName"],"sources":["/Users/abameda/Downloads/CA_QUIZZ/src/data/questions.js"],"sourcesContent":["const quizData = {\n'Chapter 1: Introduction': [\n    {\n      question: \"What is the main distinction between computer architecture and computer organization?\",\n      options: [\n        \"Architecture deals with hardware, organization deals with software\",\n        \"Architecture refers to attributes visible to programmer, organization refers to how features are implemented\",\n        \"Architecture is about performance, organization is about cost\",\n        \"There is no distinction between them\"\n      ],\n      correct: 1,\n      explanation: \"Architecture includes instruction set, data representation, I/O mechanisms - what's visible to programmers. Organization covers control signals, interfaces, memory technology - the implementation details.\"\n    },\n    {\n      question: \"Which of the following is NOT one of the four basic computer functions?\",\n      options: [\n        \"Data processing\",\n        \"Data storage\", \n        \"Data compilation\",\n        \"Data movement\"\n      ],\n      correct: 2,\n      explanation: \"The four basic functions are: data processing, data storage, data movement, and control. Data compilation is a software process, not a basic computer function.\"\n    },\n    {\n      question: \"What does the Program Counter (PC) register contain?\",\n      options: [\n        \"The current instruction being executed\",\n        \"The address of the next instruction pair to be fetched\",\n        \"The result of the last arithmetic operation\",\n        \"The memory address being accessed\"\n      ],\n      correct: 1,\n      explanation: \"The Program Counter (PC) contains the address of the next instruction pair to be fetched from memory.\"\n    },\n    {\n      question: \"In a multicore system, what is a 'core'?\",\n      options: [\n        \"The entire CPU chip\",\n        \"An individual processing unit on a processor chip\",\n        \"The system bus connecting components\",\n        \"The cache memory hierarchy\"\n      ],\n      correct: 1,\n      explanation: \"A core is an individual processing unit on a processor chip, which may be equivalent in functionality to a CPU on a single-CPU system.\"\n    },\n    {\n      question: \"What is the relationship between clock frequency and clock cycle time?\",\n      options: [\n        \"They are the same thing\",\n        \"Clock cycle time = clock frequency × 2\",\n        \"Clock cycle time = 1 / clock frequency\",\n        \"There is no mathematical relationship\"\n      ],\n      correct: 2,\n      explanation: \"Clock cycle time is the reciprocal of clock frequency. For example, an 800 MHz clock has a cycle time of 1.25 ns.\"\n    },\n    {\n      question: \"Which component manages the computer's resources and orchestrates performance?\",\n      options: [\n        \"ALU (Arithmetic Logic Unit)\",\n        \"Memory\",\n        \"Control Unit\",\n        \"Registers\"\n      ],\n      correct: 2,\n      explanation: \"The control unit manages the computer's resources and orchestrates the performance of its functional parts in response to instructions.\"\n    },\n    {\n      question: \"What is the primary purpose of cache memory?\",\n      options: [\n        \"To store the operating system\",\n        \"To backup main memory\",\n        \"To speed up memory access by storing likely-to-be-used data\",\n        \"To control input/output operations\"\n      ],\n      correct: 2,\n      explanation: \"Cache memory is smaller and faster than main memory, used to speed up memory access by placing data from main memory that is likely to be used in the near future.\"\n    },\n    {\n      question: \"Which register contains the 8-bit opcode instruction being executed?\",\n      options: [\n        \"Program Counter (PC)\",\n        \"Instruction Register (IR)\",\n        \"Memory Address Register (MAR)\",\n        \"Accumulator (AC)\"\n      ],\n      correct: 1,\n      explanation: \"The Instruction Register (IR) contains the 8-bit opcode instruction being executed.\"\n    },\n    {\n      question: \"What is the function of the Memory Address Register (MAR)?\",\n      options: [\n        \"Contains the instruction being executed\",\n        \"Stores temporary data for ALU operations\",\n        \"Specifies the address in memory of the word to be written or read\",\n        \"Controls the sequence of operations\"\n      ],\n      correct: 2,\n      explanation: \"The Memory Address Register (MAR) specifies the address in memory of the word to be written from or read into the MBR.\"\n    },\n    {\n      question: \"In embedded systems, what characterizes 'deeply embedded systems'?\",\n      options: [\n        \"They run complex operating systems\",\n        \"They are programmable after deployment\",\n        \"They are dedicated to specific tasks with extreme resource constraints\",\n        \"They always have user interfaces\"\n      ],\n      correct: 2,\n      explanation: \"Deeply embedded systems are dedicated, single-purpose devices with extreme resource constraints in terms of memory, processor size, time, and power consumption.\"\n    },\n    {\n      question: \"What are the four main structural components of a computer?\",\n      options: [\n        \"CPU, RAM, ROM, Hard Drive\",\n        \"CPU, Main Memory, I/O, System Interconnection\",\n        \"Control Unit, ALU, Registers, Cache\",\n        \"Processor, Memory, Storage, Network\"\n      ],\n      correct: 1,\n      explanation: \"The four main structural components are: CPU (controls operation and data processing), Main Memory (stores data), I/O (moves data with external environment), and System Interconnection (communication mechanism).\"\n    },\n    {\n      question: \"What is the difference between response time and throughput?\",\n      options: [\n        \"They are the same metric\",\n        \"Response time is speed, throughput is accuracy\",\n        \"Response time is time for one task, throughput is tasks per unit time\",\n        \"Response time is for CPU, throughput is for memory\"\n      ],\n      correct: 2,\n      explanation: \"Response time is the time between start and completion of a task, while throughput is the total amount of tasks done in a given time period. There is generally no relationship between these metrics.\"\n    },\n    {\n      question: \"What does the Memory Buffer Register (MBR) contain?\",\n      options: [\n        \"The address of the next instruction\",\n        \"A word to be stored in memory or received from memory/I/O\",\n        \"The current instruction being decoded\",\n        \"Control signals for the ALU\"\n      ],\n      correct: 1,\n      explanation: \"The Memory Buffer Register (MBR) contains a word to be stored in memory or sent to the I/O unit, or is used to receive a word from memory or from the I/O unit.\"\n    },\n    {\n      question: \"Which generation of IoT deployment is characterized by billions of embedded devices?\",\n      options: [\n        \"Information Technology (IT)\",\n        \"Operational Technology (OT)\",\n        \"Personal Technology\",\n        \"Sensor/Actuator Technology\"\n      ],\n      correct: 3,\n      explanation: \"The fourth generation, Sensor/Actuator Technology, is usually thought of as the IoT and is marked by the use of billions of embedded devices using wireless connectivity.\"\n    },\n    {\n      question: \"What is the primary function of the Instruction Buffer Register (IBR)?\",\n      options: [\n        \"To store the program counter value\",\n        \"To temporarily hold the right-hand instruction from memory\",\n        \"To contain the address of data in memory\",\n        \"To store the result of arithmetic operations\"\n      ],\n      correct: 1,\n      explanation: \"The Instruction Buffer Register (IBR) is employed to temporarily hold the right-hand instruction from a word in memory.\"\n    },\n    {\n      question: \"In the context of data movement, what distinguishes I/O from data communications?\",\n      options: [\n        \"Speed of data transfer\",\n        \"Type of data being moved\",\n        \"Distance - I/O is direct connection, data communications is over longer distances\",\n        \"Security requirements\"\n      ],\n      correct: 2,\n      explanation: \"I/O occurs when data are received from or delivered to a device directly connected to the computer, while data communications involves moving data over longer distances to or from remote devices.\"\n    },\n    {\n      question: \"What is the key difference between application processors and dedicated processors in embedded systems?\",\n      options: [\n        \"Power consumption levels\",\n        \"Manufacturing cost\",\n        \"Application processors execute complex OS, dedicated processors serve specific tasks\",\n        \"Physical size differences\"\n      ],\n      correct: 2,\n      explanation: \"Application processors are defined by their ability to execute complex operating systems and are general-purpose, while dedicated processors are dedicated to one or a small number of specific tasks.\"\n    },\n    {\n      question: \"Which components provide the four basic functions of integrated circuits?\",\n      options: [\n        \"Transistors, resistors, capacitors, inductors\",\n        \"Gates (processing), memory cells (storage), paths (movement), control signals (control)\",\n        \"CPU, memory, I/O, interconnection\",\n        \"Hardware, software, firmware, middleware\"\n      ],\n      correct: 1,\n      explanation: \"In integrated circuits: gates provide data processing, memory cells provide data storage, paths provide data movement, and control signals provide control functions.\"\n    },\n    {\n      question: \"What advantage do Intel x86 and IBM System/370 families demonstrate?\",\n      options: [\n        \"Higher performance than competitors\",\n        \"Lower manufacturing costs\",\n        \"Code compatibility through shared basic architecture\",\n        \"Better power efficiency\"\n      ],\n      correct: 2,\n      explanation: \"All Intel x86 family and IBM System/370 family share the same basic architecture, which gives code compatibility (at least backwards), even though organization differs between versions.\"\n    },\n    {\n      question: \"In deeply embedded systems, what type of processor is typically used?\",\n      options: [\n        \"Microprocessor\",\n        \"Microcontroller\",\n        \"Graphics processor\",\n        \"Digital signal processor\"\n      ],\n      correct: 1,\n      explanation: \"Deeply embedded systems use a microcontroller rather than a microprocessor, are not programmable once the program logic has been burned into ROM, and have no interaction with a user.\"\n    },\n    {\n      question: \"What are the major structural components of a CPU?\",\n      options: [\n        \"Cache, registers, bus, clock\",\n        \"Control Unit, ALU, Registers, CPU Interconnection\",\n        \"Fetch unit, decode unit, execute unit, write-back unit\",\n        \"L1 cache, L2 cache, L3 cache, main memory interface\"\n      ],\n      correct: 1,\n      explanation: \"The CPU's major structural components are: Control Unit (controls CPU operation), ALU (performs data processing), Registers (provide internal storage), and CPU Interconnection (provides communication among components).\"\n    },\n    {\n      question: \"What characterizes embedded systems' interaction with their environment?\",\n      options: [\n        \"They operate independently of external factors\",\n        \"They are tightly coupled with real-time constraints\",\n        \"They only process stored data\",\n        \"They communicate only through network interfaces\"\n      ],\n      correct: 1,\n      explanation: \"Embedded systems are often tightly coupled to their environment, giving rise to real-time constraints imposed by the need to interact with the environment, such as required speeds, precision, and timing.\"\n    }\n  ],\n  'Chapter 2: Storage Environment and RAID': [\n  {\n    question: \"What is the main difference between memory modules and storage devices?\",\n    options: [\n      \"Memory modules are cheaper than storage devices\",\n      \"Memory modules use semiconductor chips while storage devices use magnetic or optical media\",\n      \"Storage devices are faster than memory modules\",\n      \"Memory modules are non-volatile while storage devices are volatile\"\n    ],\n    correct: 1,\n    explanation: \"Memory modules are implemented using semiconductor chips, whereas storage devices use either magnetic or optical media. Memory modules also enable data access at higher speed than storage media.\"\n  },\n  {\n    question: \"Which type of memory is volatile and requires constant power supply?\",\n    options: [\n      \"ROM (Read-Only Memory)\",\n      \"RAM (Random Access Memory)\", \n      \"Hard disk storage\",\n      \"CD-ROM storage\"\n    ],\n    correct: 1,\n    explanation: \"RAM is volatile and requires a constant supply of power to maintain memory cell content. Data is erased when the system's power is turned off or interrupted.\"\n  },\n  {\n    question: \"What does DAS stand for and how does it connect to servers?\",\n    options: [\n      \"Direct Access Storage - connects via network protocols\",\n      \"Distributed Array Storage - connects via fiber optic cables\",\n      \"Direct Attached Storage - connects directly through Host Bus Adapter (HBA)\",\n      \"Dynamic Allocation Storage - connects via USB only\"\n    ],\n    correct: 2,\n    explanation: \"DAS stands for Direct Attached Storage. It connects directly to a server through Host Bus Adapter (HBA), with no network between storage and host servers.\"\n  },\n  {\n    question: \"What is the main advantage of DAS over other storage environments?\",\n    options: [\n      \"Accessibility from multiple devices\",\n      \"Fast performance and simple setup\",\n      \"Network sharing capabilities\",\n      \"Lowest cost among all options\"\n    ],\n    correct: 1,\n    explanation: \"DAS offers fast performance, high capacity options, and simple setup as its main advantages.\"\n  },\n  {\n    question: \"What is the primary disadvantage of DAS?\",\n    options: [\n      \"Slow performance\",\n      \"High cost\",\n      \"Not accessible from other devices and prone to data loss if connected computer fails\",\n      \"Complex setup requirements\"\n    ],\n    correct: 2,\n    explanation: \"DAS is not accessible from other devices and is prone to data loss if the connected computer fails.\"\n  },\n  {\n    question: \"What does SAN stand for and what is its primary characteristic?\",\n    options: [\n      \"Storage Access Network - a wireless storage solution\",\n      \"System Area Network - connects only servers\",\n      \"Storage Area Networks - a dedicated data storage network accessible by multiple servers\",\n      \"Secure Access Network - provides encrypted storage\"\n    ],\n    correct: 2,\n    explanation: \"SAN stands for Storage Area Networks. It is a dedicated data storage network which can be accessed by multiple servers.\"\n  },\n  {\n    question: \"Which storage environment offers the fastest performance?\",\n    options: [\n      \"DAS\",\n      \"SAN\",\n      \"NAS\",\n      \"RAID\"\n    ],\n    correct: 1,\n    explanation: \"SAN offers the fastest performance among the options due to its dedicated network specifically designed for high-performance connections.\"\n  },\n  {\n    question: \"What are the main protocols used in SAN?\",\n    options: [\n      \"TCP/IP and HTTP\",\n      \"SCSI and SATA\",\n      \"USB and FireWire\",\n      \"Ethernet and WiFi\"\n    ],\n    correct: 1,\n    explanation: \"The protocols that are used in SAN are SCSI and SATA.\"\n  },\n  {\n    question: \"What is the main weakness of SAN mentioned in the document?\",\n    options: [\n      \"Speed and Latency\",\n      \"Security\",\n      \"Cost\",\n      \"Complexity\"\n    ],\n    correct: 1,\n    explanation: \"According to the document, the weakness of SAN is Security, while Speed and Latency are listed as disadvantages.\"\n  },\n  {\n    question: \"What does NAS stand for and at what level does it operate?\",\n    options: [\n      \"Network Access Storage - operates at block level\",\n      \"Network Attached Storage - operates at file level\",\n      \"Network Array Storage - operates at bit level\",\n      \"Network Administration Storage - operates at system level\"\n    ],\n    correct: 1,\n    explanation: \"NAS stands for Network Attached Storage. It is a file level computer data storage that connects to other devices on TCP/IP network.\"\n  },\n  {\n    question: \"Which component is NOT part of NAS architecture?\",\n    options: [\n      \"Head unit (CPU, Memory)\",\n      \"Network Interface Card (NIC)\",\n      \"Host Bus Adapter (HBA)\",\n      \"Optimized operating system\"\n    ],\n    correct: 2,\n    explanation: \"Host Bus Adapter (HBA) is a component of DAS, not NAS. NAS components include Head unit, NIC, optimized OS, protocols, and storage protocols.\"\n  },\n  {\n    question: \"What does RAID stand for?\",\n    options: [\n      \"Random Array of Independent Disks\",\n      \"Redundant Array of Independent Disks\",\n      \"Reliable Array of Integrated Disks\",\n      \"Rapid Access of Internal Disks\"\n    ],\n    correct: 1,\n    explanation: \"RAID stands for Redundant Array of Independent Disks.\"\n  },\n  {\n    question: \"Which RAID function writes consecutive logical blocks on consecutive physical disks?\",\n    options: [\n      \"Mirroring\",\n      \"Striping\",\n      \"Parity Calculation\",\n      \"Synchronization\"\n    ],\n    correct: 1,\n    explanation: \"Striping writes consecutive logical byte/blocks on consecutive physical disks.\"\n  },\n  {\n    question: \"Which RAID level provides striping with no parity or redundancy?\",\n    options: [\n      \"RAID 1\",\n      \"RAID 0\",\n      \"RAID 5\",\n      \"RAID 10\"\n    ],\n    correct: 1,\n    explanation: \"RAID 0 provides stripe with no parity, offering excellent performance but no redundancy.\"\n  },\n  {\n    question: \"What is the minimum number of disks required for RAID 1?\",\n    options: [\n      \"1 disk\",\n      \"2 disks\",\n      \"3 disks\",\n      \"4 disks\"\n    ],\n    correct: 1,\n    explanation: \"RAID 1 requires a minimum of 2 disks and provides excellent redundancy as blocks are mirrored.\"\n  },\n  {\n    question: \"Which RAID level is described as 'stripe of mirrors'?\",\n    options: [\n      \"RAID 0+1\",\n      \"RAID 5\",\n      \"RAID 10\",\n      \"RAID 6\"\n    ],\n    correct: 2,\n    explanation: \"RAID 10 is also called 'stripe of mirrors' and requires a minimum of 4 disks.\"\n  },\n  {\n    question: \"What is the minimum number of disks required for RAID 5?\",\n    options: [\n      \"2 disks\",\n      \"3 disks\",\n      \"4 disks\",\n      \"5 disks\"\n    ],\n    correct: 1,\n    explanation: \"RAID 5 requires a minimum of 3 disks and provides good performance with distributed parity.\"\n  },\n  {\n    question: \"Which RAID level uses byte-level striping?\",\n    options: [\n      \"RAID 3\",\n      \"RAID 4\",\n      \"RAID 5\",\n      \"RAID 6\"\n    ],\n    correct: 0,\n    explanation: \"RAID 3 uses byte level striping, while RAID 4 uses block level striping.\"\n  },\n  {\n    question: \"What makes RAID 6 different from RAID 5?\",\n    options: [\n      \"It uses striping instead of mirroring\",\n      \"It has double parity blocks and requires 6 disks\",\n      \"It provides better performance\",\n      \"It uses fewer disks\"\n    ],\n    correct: 1,\n    explanation: \"RAID 6 is similar to RAID 5 except it has double parity blocks and requires 6 disks.\"\n  },\n  {\n    question: \"Which RAID level is recommended for heavily read-oriented databases?\",\n    options: [\n      \"RAID 0\",\n      \"RAID 1\",\n      \"RAID 5\",\n      \"RAID 10\"\n    ],\n    correct: 2,\n    explanation: \"RAID 5 is the best cost effective option providing both performance and redundancy, recommended for databases that are heavily read oriented, though write operations will be slow.\"\n  },\n  {\n    question: \"Which RAID level is considered the BEST option for mission critical applications?\",\n    options: [\n      \"RAID 0\",\n      \"RAID 5\",\n      \"RAID 6\",\n      \"RAID 10\"\n    ],\n    correct: 3,\n    explanation: \"RAID 10 provides excellent redundancy and excellent performance. If you can afford it, this is the BEST option for any mission critical applications, especially databases.\"\n  },\n  {\n    question: \"Which storage environment would be best for a single user storing personal files?\",\n    options: [\n      \"DAS\",\n      \"SAN\",\n      \"NAS\",\n      \"RAID\"\n    ],\n    correct: 0,\n    explanation: \"DAS is best for individual workstations and storing personal data due to its fast performance and simple setup.\"\n  },\n  {\n    question: \"Which storage environment is most suitable for small businesses with multiple employees sharing files?\",\n    options: [\n      \"DAS\",\n      \"SAN\", \n      \"NAS\",\n      \"RAID\"\n    ],\n    correct: 2,\n    explanation: \"NAS is best for small businesses and workgroups as it provides shared storage accessible from multiple devices on the network.\"\n  },\n  {\n    question: \"Which storage environment requires the most technical expertise to manage?\",\n    options: [\n      \"DAS\",\n      \"SAN\",\n      \"NAS\", \n      \"RAID\"\n    ],\n    correct: 1,\n    explanation: \"SAN requires significant technical expertise to manage due to its specialized hardware and complex configuration.\"\n  },\n  {\n    question: \"In terms of cost ranking from lowest to highest, what is the correct order?\",\n    options: [\n      \"SAN, NAS, DAS\",\n      \"DAS, NAS, SAN\",\n      \"NAS, DAS, SAN\",\n      \"DAS, SAN, NAS\"\n    ],\n    correct: 1,\n    explanation: \"DAS is generally the cheapest, followed by NAS, then SAN is the most expensive due to specialized hardware and configuration.\"\n  },\n  {\n    question: \"Which statement about RAID is TRUE?\",\n    options: [\n      \"RAID guarantees complete data protection against any failure\",\n      \"All RAID levels offer the same level of performance and redundancy\",\n      \"RAID can be implemented on both DAS and NAS systems\",\n      \"Using RAID eliminates the need for backups\"\n    ],\n    correct: 2,\n    explanation: \"RAID can be implemented on both DAS and NAS systems for added benefits. RAID does not guarantee complete protection, levels differ in performance/redundancy, and backups are still needed.\"\n  },\n  {\n    question: \"What type of data access does SAN provide?\",\n    options: [\n      \"File level data access\",\n      \"Block level data storage\",\n      \"Byte level data access\",\n      \"Application level data access\"\n    ],\n    correct: 1,\n    explanation: \"SAN provides block level data storage, allowing multiple clients to access files at the same time with very high performance.\"\n  },\n  {\n    question: \"Which benefit is NOT associated with NAS?\",\n    options: [\n      \"Relatively inexpensive\",\n      \"24/7 and remote data availability\",\n      \"Fastest performance among storage options\",\n      \"Automatic backups to other devices and cloud\"\n    ],\n    correct: 2,\n    explanation: \"NAS has slower performance compared to DAS due to network overhead. The fastest performance is provided by SAN.\"\n  },\n  {\n    question: \"What happens to RAM data when power is interrupted?\",\n    options: [\n      \"Data is automatically backed up\",\n      \"Data remains intact\",\n      \"Data is erased\",\n      \"Data is transferred to ROM\"\n    ],\n    correct: 2,\n    explanation: \"RAM is volatile and requires constant power supply. Data is erased when the system's power is turned off or interrupted.\"\n  },\n  {\n    question: \"Which RAID level should NOT be used for critical systems?\",\n    options: [\n      \"RAID 0\",\n      \"RAID 1\", \n      \"RAID 5\",\n      \"RAID 10\"\n    ],\n    correct: 0,\n    explanation: \"RAID 0 has no redundancy (no mirror, no parity), so it should not be used for any critical system despite its excellent performance.\"\n  }\n],\n'Chapter 3: Computer Architecture Introduction and Basics': [\n  {\n    question: \"What is the main purpose of computing according to Richard Hamming?\",\n    options: [\n      \"To generate numbers and data\",\n      \"To provide insight, not just numbers\",\n      \"To execute instructions efficiently\",\n      \"To solve mathematical equations\"\n    ],\n    correct: 1,\n    explanation: \"According to Richard Hamming, 'The purpose of computing is insight, not numbers.' The true value of computing lies in using data to gain understanding and knowledge (insight).\"\n  },\n  {\n    question: \"Which transformation level is directly above Logic in the hierarchy?\",\n    options: [\n      \"Circuits\",\n      \"Microarchitecture\", \n      \"ISA (Architecture)\",\n      \"Runtime System\"\n    ],\n    correct: 1,\n    explanation: \"In the levels of transformation hierarchy, Microarchitecture is directly above Logic, which is above Circuits.\"\n  },\n  {\n    question: \"What is abstraction in the context of computer systems?\",\n    options: [\n      \"A method to hide implementation details from users\",\n      \"A higher level only needs to know the interface to the lower level, not how it's implemented\",\n      \"A way to make programs run faster\",\n      \"A technique for optimizing hardware design\"\n    ],\n    correct: 1,\n    explanation: \"Abstraction means a higher level only needs to know about the interface to the lower level, not how the lower level is implemented.\"\n  },\n  {\n    question: \"Why might you need to understand what happens in underlying abstraction levels?\",\n    options: [\n      \"Only for academic purposes\",\n      \"When programs run slow, consume too much energy, or don't run correctly\",\n      \"To impress colleagues with technical knowledge\",\n      \"It's never necessary in modern computing\"\n    ],\n    correct: 1,\n    explanation: \"You need to understand underlying levels when programs run slow, consume too much energy, don't run correctly, or when designing more efficient systems.\"\n  },\n  {\n    question: \"What are the two key goals of the computer architecture course mentioned?\",\n    options: [\n      \"Learn programming and hardware design\",\n      \"Understand processor internals and make cross-layer optimization decisions\",\n      \"Master assembly language and digital logic\",\n      \"Study algorithms and data structures\"\n    ],\n    correct: 1,\n    explanation: \"The two key goals are: understand how a processor works underneath the software layer and how hardware decisions affect software/programmers, and enable making design decisions that cross boundaries of different layers.\"\n  },\n  {\n    question: \"In the multi-core system slowdown example, what causes the disparity in performance between applications?\",\n    options: [\n      \"CPU scheduling policies\",\n      \"Cache miss rates\",\n      \"DRAM controller scheduling unfairness\",\n      \"Network latency\"\n    ],\n    correct: 2,\n    explanation: \"The disparity is caused by DRAM controller scheduling policies that are unfair to some applications, particularly the row-hit first and oldest-first policies.\"\n  },\n  {\n    question: \"What does a row-conflict memory access result in compared to a row-hit access?\",\n    options: [\n      \"Faster access time\",\n      \"Same access time\",\n      \"Significantly longer access time\",\n      \"Slightly longer access time\"\n    ],\n    correct: 2,\n    explanation: \"A row-conflict memory access takes significantly longer than a row-hit access.\"\n  },\n  {\n    question: \"What does FR-FCFS stand for in DRAM controller scheduling?\",\n    options: [\n      \"Fast-Ready, First-Come-First-Service\",\n      \"First-Ready, First-Come-First-Service\", \n      \"First-Row, First-Column-First-Service\",\n      \"Fast-Row, First-Cache-First-Service\"\n    ],\n    correct: 1,\n    explanation: \"FR-FCFS stands for First-Ready, First-Come-First-Service, which prioritizes row-hit accesses first, then older accesses first.\"\n  },\n  {\n    question: \"Which application type does the row-hit first policy unfairly prioritize?\",\n    options: [\n      \"CPU-intensive applications\",\n      \"Applications with high row buffer locality\",\n      \"Applications with random memory access\",\n      \"Applications with low memory usage\"\n    ],\n    correct: 1,\n    explanation: \"Row-hit first policy unfairly prioritizes applications with high row buffer locality - threads that keep accessing the same row.\"\n  },\n  {\n    question: \"What is the difference between STREAM and RANDOM memory access patterns?\",\n    options: [\n      \"STREAM is faster, RANDOM is slower\",\n      \"STREAM has sequential access with high row buffer locality, RANDOM has random access with low row buffer locality\",\n      \"STREAM uses more memory, RANDOM uses less\",\n      \"STREAM is for reading, RANDOM is for writing\"\n    ],\n    correct: 1,\n    explanation: \"STREAM has sequential memory access with very high row buffer locality (96% hit rate), while RANDOM has random memory access with very low row buffer locality (3% hit rate).\"\n  },\n  {\n    question: \"What does a DRAM cell consist of?\",\n    options: [\n      \"Two transistors and a resistor\",\n      \"A capacitor and an access transistor\",\n      \"A flip-flop circuit\",\n      \"Multiple logic gates\"\n    ],\n    correct: 1,\n    explanation: \"A DRAM cell consists of a capacitor and an access transistor. It stores data in terms of charge in the capacitor.\"\n  },\n  {\n    question: \"Why does DRAM need to be refreshed periodically?\",\n    options: [\n      \"To prevent data corruption from electromagnetic interference\",\n      \"Because the capacitor charge leaks over time\",\n      \"To maintain synchronization with the CPU clock\",\n      \"To update the stored data\"\n    ],\n    correct: 1,\n    explanation: \"DRAM capacitor charge leaks over time, so the memory controller needs to refresh each row periodically to restore charge.\"\n  },\n  {\n    question: \"What is the typical refresh period for DRAM?\",\n    options: [\n      \"64 microseconds\",\n      \"64 milliseconds\",\n      \"64 seconds\", \n      \"64 nanoseconds\"\n    ],\n    correct: 1,\n    explanation: \"The typical refresh period is 64 ms - each row needs to be activated every 64 milliseconds.\"\n  },\n  {\n    question: \"What are the downsides of DRAM refresh mentioned in the document?\",\n    options: [\n      \"Only energy consumption\",\n      \"Energy consumption, performance degradation, QoS impact, and capacity scaling limits\",\n      \"Only performance degradation\",\n      \"Only capacity limitations\"\n    ],\n    correct: 1,\n    explanation: \"DRAM refresh downsides include: energy consumption, performance degradation (DRAM unavailable during refresh), QoS/predictability impact (pause times), and refresh rate limits DRAM capacity scaling.\"\n  },\n  {\n    question: \"According to the RAIDR research, what percentage of refresh reduction was achieved?\",\n    options: [\n      \"46.8%\",\n      \"74.6%\",\n      \"15.2%\",\n      \"84.3%\"\n    ],\n    correct: 1,\n    explanation: \"RAIDR achieved 74.6% refresh reduction with only 1.25KB storage overhead for 32GB memory.\"\n  },\n  {\n    question: \"What is the key observation behind the RAIDR approach?\",\n    options: [\n      \"All DRAM rows need frequent refresh\",\n      \"Most DRAM rows can be refreshed much less often without losing data\",\n      \"DRAM refresh is unnecessary\",\n      \"Refresh frequency should be increased\"\n    ],\n    correct: 1,\n    explanation: \"RAIDR's key observation is that most DRAM rows can be refreshed much less often without losing data, allowing for different refresh rates for different rows.\"\n  },\n  {\n    question: \"What are the three main steps in the RAIDR approach?\",\n    options: [\n      \"Reading, Writing, Refreshing\",\n      \"Profiling, Binning, Refreshing\",\n      \"Measuring, Sorting, Optimizing\",\n      \"Testing, Grouping, Scheduling\"\n    ],\n    correct: 1,\n    explanation: \"RAIDR uses three steps: 1) Profiling retention time of all rows, 2) Binning rows by retention time using Bloom Filters, 3) Refreshing rows in different bins at different rates.\"\n  },\n  {\n    question: \"How much storage overhead does RAIDR require for 32GB memory?\",\n    options: [\n      \"1.25MB\",\n      \"1.25KB\",\n      \"12.5KB\",\n      \"125KB\"\n    ],\n    correct: 1,\n    explanation: \"RAIDR requires only 1.25KB storage for 32GB memory using efficient storage with Bloom Filters.\"\n  },\n  {\n    question: \"What performance improvement did RAIDR achieve?\",\n    options: [\n      \"~5%\",\n      \"~9%\",\n      \"~16%\",\n      \"~20%\"\n    ],\n    correct: 1,\n    explanation: \"RAIDR achieved approximately 9% performance improvement along with 16%/20% DRAM dynamic/idle power reduction.\"\n  },\n  {\n    question: \"In the memory hog example, what happens when T0 (STREAM) and T1 (RANDOM) compete for memory access?\",\n    options: [\n      \"They get equal access\",\n      \"T1 gets prioritized due to oldest-first policy\",\n      \"128 requests of T0 are serviced before T1 due to row buffer locality\",\n      \"Random scheduling occurs\"\n    ],\n    correct: 2,\n    explanation: \"Due to row buffer locality, 128 requests of T0 (STREAM) are serviced before T1 (RANDOM) gets access, since T0 keeps hitting the same row while T1 causes row conflicts.\"\n  },\n  {\n    question: \"What is the main takeaway about abstraction layers from this chapter?\",\n    options: [\n      \"Abstraction layers should never be broken\",\n      \"Breaking abstraction layers and knowing what's underneath enables problem solving and better system design\",\n      \"Only hardware designers need to understand multiple layers\",\n      \"Abstraction layers are only important for software development\"\n    ],\n    correct: 1,\n    explanation: \"The main takeaway is that breaking abstraction layers and knowing what happens underneath enables you to solve problems and design better future systems.\"\n  },\n  {\n    question: \"Which component in a multi-core system is shared among all cores?\",\n    options: [\n      \"L2 Cache\",\n      \"L3 Cache and DRAM Memory Controller\",\n      \"CPU registers\",\n      \"Instruction decoder\"\n    ],\n    correct: 1,\n    explanation: \"In the multi-core system diagram, the Shared L3 Cache and DRAM Memory Controller are shared among all cores, while each core has its own L2 cache.\"\n  },\n  {\n    question: \"What makes the DRAM controller vulnerable to denial of service attacks?\",\n    options: [\n      \"Poor encryption mechanisms\",\n      \"Unfair scheduling policies that can be exploited by specially written programs\",\n      \"Insufficient bandwidth\",\n      \"Hardware design flaws\"\n    ],\n    correct: 1,\n    explanation: \"DRAM scheduling policies are unfair to some applications, and programs can be written to exploit this unfairness, making the controller vulnerable to denial of service attacks.\"\n  },\n  {\n    question: \"What is the row size mentioned in the memory hog example?\",\n    options: [\n      \"4KB\",\n      \"8KB\",\n      \"16KB\",\n      \"32KB\"\n    ],\n    correct: 1,\n    explanation: \"In the memory hog example, the row size is 8KB and cache block size is 64B, resulting in 128 (8KB/64B) requests.\"\n  },\n  {\n    question: \"According to the refresh overhead graphs, what percentage of performance overhead can refresh cause?\",\n    options: [\n      \"Up to 8%\",\n      \"Up to 46%\",\n      \"Up to 15%\",\n      \"Up to 47%\"\n    ],\n    correct: 1,\n    explanation: \"According to the refresh overhead performance graph, DRAM refresh can cause up to 46% performance overhead.\"\n  },\n  {\n    question: \"What cooperation approach does the chapter suggest for solving complex system problems?\",\n    options: [\n      \"Hardware-only solutions\",\n      \"Software-only solutions\",\n      \"Cooperation between multiple components and layers\",\n      \"Operating system level solutions only\"\n    ],\n    correct: 2,\n    explanation: \"The chapter emphasizes that cooperation between multiple components and layers can enable more effective solutions and systems.\"\n  },\n  {\n    question: \"In the levels of transformation, what is at the bottom of the hierarchy?\",\n    options: [\n      \"Logic\",\n      \"Circuits\",\n      \"Electrons\",\n      \"Microarchitecture\"\n    ],\n    correct: 2,\n    explanation: \"In the levels of transformation hierarchy, Electrons is at the bottom, representing the most fundamental physical level.\"\n  },\n  {\n    question: \"What does the course aim to enable students to do regarding design decisions?\",\n    options: [\n      \"Make decisions within single layers only\",\n      \"Focus only on software optimization\",\n      \"Make design and optimization decisions that cross boundaries of different layers\",\n      \"Specialize in one specific layer\"\n    ],\n    correct: 2,\n    explanation: \"The course aims to enable students to be comfortable making design and optimization decisions that cross the boundaries of different layers and system components.\"\n  },\n  {\n    question: \"What information does RAIDR expose to solve the refresh problem?\",\n    options: [\n      \"CPU utilization patterns\",\n      \"Retention time profile information of DRAM rows\",\n      \"Cache miss rates\",\n      \"Network traffic patterns\"\n    ],\n    correct: 1,\n    explanation: \"RAIDR exposes retention time profile information of DRAM rows to the memory controller, enabling different refresh rates for different rows based on their retention characteristics.\"\n  },\n  {\n    question: \"What is the relationship between the programmer's view and hardware designer's view in computer systems?\",\n    options: [\n      \"They are completely independent\",\n      \"Only the programmer's view matters\",\n      \"The architect/microarchitect's choices critically affect both views\",\n      \"Only the hardware designer's view is important\"\n    ],\n    correct: 2,\n    explanation: \"The architect/microarchitect's view involves designing computers that meet system design goals, and these choices critically affect both the software programmer and the hardware designer.\"\n  }\n],\n'Chapter 4: Introduction and Basics': [\n  {\n    question: \"According to Richard Hamming, what is the purpose of computing?\",\n    options: [\n      \"To generate numbers and data\",\n      \"To solve mathematical equations\",\n      \"To gain insight, not numbers\",\n      \"To process information quickly\"\n    ],\n    correct: 2,\n    explanation: \"Richard Hamming stated that 'The purpose of computing is insight, not numbers'. The true value of computing lies not just in generating numbers (data), but in using that data to gain understanding and knowledge (insight).\"\n  },\n  {\n    question: \"What are the levels of transformation in computer systems from top to bottom?\",\n    options: [\n      \"Problem → Algorithm → Program → ISA → Microarchitecture → Logic → Circuits → Electrons\",\n      \"Algorithm → Problem → Program → ISA → Logic → Microarchitecture → Circuits → Electrons\",\n      \"Problem → Program → Algorithm → ISA → Microarchitecture → Logic → Circuits → Electrons\",\n      \"Problem → Algorithm → ISA → Program → Microarchitecture → Logic → Circuits → Electrons\"\n    ],\n    correct: 0,\n    explanation: \"The correct hierarchy is: Problem → Algorithm → Program/Language → Runtime System → ISA (Architecture) → Microarchitecture → Logic → Circuits → Electrons, representing the transformation from high-level problems to physical implementation.\"\n  },\n  {\n    question: \"What is abstraction in the context of computer systems?\",\n    options: [\n      \"A method to make systems more complex\",\n      \"A higher level only needs to know about the interface to the lower level, not how it's implemented\",\n      \"A way to combine multiple levels into one\",\n      \"A technique to eliminate unnecessary components\"\n    ],\n    correct: 1,\n    explanation: \"Abstraction means that a higher level only needs to know about the interface to the lower level, not how the lower level is implemented. For example, a high-level language programmer doesn't need to know what the ISA is or how a computer executes instructions.\"\n  },\n  {\n    question: \"Why might you need to understand what happens in underlying abstraction layers?\",\n    options: [\n      \"Only for academic purposes\",\n      \"When programs run slow, incorrectly, or consume too much energy\",\n      \"To make programming more difficult\",\n      \"It's never necessary to understand underlying layers\"\n    ],\n    correct: 1,\n    explanation: \"Understanding underlying layers becomes crucial when: the program runs slow, doesn't run correctly, consumes too much energy, or when designing more efficient and higher performance systems.\"\n  },\n  {\n    question: \"In the multi-core system memory performance attack example, what causes the disparity in slowdowns between applications?\",\n    options: [\n      \"Different CPU speeds\",\n      \"Cache size differences\",\n      \"DRAM scheduling policy unfairness due to row buffer locality\",\n      \"Operating system scheduling\"\n    ],\n    correct: 2,\n    explanation: \"The disparity is caused by DRAM scheduling policies being unfair to some applications. Row-hit first policy unfairly prioritizes apps with high row buffer locality, while oldest-first unfairly prioritizes memory-intensive applications.\"\n  },\n  {\n    question: \"What is the FR-FCFS scheduling policy in DRAM controllers?\",\n    options: [\n      \"First-Request, First-Come-First-Service\",\n      \"First-Ready, First-Come-First-Service\",\n      \"First-Row, First-Column-First-Service\",\n      \"Fast-Response, First-Come-First-Service\"\n    ],\n    correct: 1,\n    explanation: \"FR-FCFS stands for First-Ready, First-Come-First-Service. It has two rules: (1) Row-hit first: Service row-hit memory accesses first, (2) Oldest-first: Then service older accesses first.\"\n  },\n  {\n    question: \"What makes a DRAM row-conflict access significantly slower than a row-hit access?\",\n    options: [\n      \"CPU processing delays\",\n      \"Cache miss penalties\",\n      \"Need to close current row and open new row in DRAM\",\n      \"Network latency\"\n    ],\n    correct: 2,\n    explanation: \"A row-conflict access requires closing the currently open row and opening a new row in DRAM, which takes significantly longer than accessing data from an already open row (row-hit).\"\n  },\n  {\n    question: \"In the memory performance hog example, what characterizes the STREAM access pattern?\",\n    options: [\n      \"Random memory access with low row buffer locality\",\n      \"Sequential memory access with very high row buffer locality (96% hit rate)\",\n      \"Scattered memory access with medium locality\",\n      \"Circular memory access pattern\"\n    ],\n    correct: 1,\n    explanation: \"STREAM is characterized by sequential memory access with very high row buffer locality (96% hit rate) and is memory intensive, making it a memory performance hog.\"\n  },\n  {\n    question: \"What characterizes the RANDOM access pattern in the memory performance example?\",\n    options: [\n      \"Sequential access with high locality\",\n      \"Random memory access with very low row buffer locality (3% hit rate)\",\n      \"Structured access with medium locality\",\n      \"Predictable access pattern\"\n    ],\n    correct: 1,\n    explanation: \"RANDOM is characterized by random memory access with very low row buffer locality (3% hit rate) and is similarly memory intensive compared to STREAM.\"\n  },\n  {\n    question: \"What does a DRAM cell consist of?\",\n    options: [\n      \"Two transistors and a resistor\",\n      \"A capacitor and an access transistor\",\n      \"Three capacitors in series\",\n      \"A flip-flop circuit\"\n    ],\n    correct: 1,\n    explanation: \"A DRAM cell consists of a capacitor and an access transistor. It stores data in terms of charge in the capacitor.\"\n  },\n  {\n    question: \"Why does DRAM need to be refreshed?\",\n    options: [\n      \"To improve performance\",\n      \"Because capacitor charge leaks over time\",\n      \"To reduce power consumption\",\n      \"To increase storage capacity\"\n    ],\n    correct: 1,\n    explanation: \"DRAM capacitor charge leaks over time, so the memory controller needs to refresh each row periodically to restore charge. Typically each row must be refreshed every 64ms.\"\n  },\n  {\n    question: \"What are the main downsides of DRAM refresh?\",\n    options: [\n      \"Only increased cost\",\n      \"Only performance degradation\",\n      \"Energy consumption, performance degradation, QoS impact, and capacity scaling limits\",\n      \"Only energy consumption\"\n    ],\n    correct: 2,\n    explanation: \"DRAM refresh has multiple downsides: energy consumption (each refresh consumes energy), performance degradation (DRAM unavailable while refreshed), QoS/predictability impact (long pause times), and refresh rate limits DRAM capacity scaling.\"\n  },\n  {\n    question: \"What is the typical refresh period for DRAM rows?\",\n    options: [\n      \"64 microseconds\",\n      \"64 milliseconds\",\n      \"64 seconds\",\n      \"64 nanoseconds\"\n    ],\n    correct: 1,\n    explanation: \"The typical refresh period is 64 milliseconds (64 ms). Each row must be activated (refreshed) every 64 ms to restore the charge in the capacitors.\"\n  },\n  {\n    question: \"What key observation does RAIDR make about DRAM refresh?\",\n    options: [\n      \"All rows need frequent refresh\",\n      \"Most DRAM rows can be refreshed much less often without losing data\",\n      \"Refresh is unnecessary\",\n      \"Only some rows need any refresh\"\n    ],\n    correct: 1,\n    explanation: \"RAIDR observes that most DRAM rows can be refreshed much less often without losing data, leading to the idea of refreshing rows containing weak cells more frequently and other rows less frequently.\"\n  },\n  {\n    question: \"How does RAIDR achieve refresh reduction?\",\n    options: [\n      \"By eliminating refresh entirely\",\n      \"By profiling retention times, binning rows, and refreshing different bins at different rates\",\n      \"By using different DRAM technology\",\n      \"By increasing refresh frequency for all rows\"\n    ],\n    correct: 1,\n    explanation: \"RAIDR works in three steps: (1) Profiling retention time of all rows, (2) Binning rows by retention time in memory controller using Bloom Filters, (3) Refreshing rows in different bins at different rates.\"\n  },\n  {\n    question: \"What are the benefits achieved by RAIDR?\",\n    options: [\n      \"Only performance improvement\",\n      \"74.6% refresh reduction, ~16%/20% DRAM power reduction, ~9% performance improvement\",\n      \"Only power reduction\",\n      \"Only refresh reduction\"\n    ],\n    correct: 1,\n    explanation: \"RAIDR achieves multiple benefits: 74.6% refresh reduction with only 1.25KB storage overhead, ~16%/20% DRAM dynamic/idle power reduction, and ~9% performance improvement, with benefits increasing with DRAM capacity.\"\n  },\n  {\n    question: \"What is one of the two key goals of the computer architecture course mentioned in the lecture?\",\n    options: [\n      \"To learn programming languages\",\n      \"To understand how a processor works underneath the software layer\",\n      \"To design operating systems\",\n      \"To build hardware components\"\n    ],\n    correct: 1,\n    explanation: \"One key goal is to understand how a processor works underneath the software layer and how decisions made in hardware affect the software/programmer.\"\n  },\n  {\n    question: \"What is the second key goal of the computer architecture course?\",\n    options: [\n      \"To memorize instruction sets\",\n      \"To enable making design and optimization decisions that cross boundaries of different layers\",\n      \"To focus only on hardware design\",\n      \"To specialize in one abstraction layer\"\n    ],\n    correct: 1,\n    explanation: \"The second key goal is to enable students to be comfortable in making design and optimization decisions that cross the boundaries of different layers and system components.\"\n  },\n  {\n    question: \"In the memory performance hog scenario, approximately how many requests of the STREAM application (T0) are serviced before the RANDOM application (T1) gets served?\",\n    options: [\n      \"64 requests\",\n      \"96 requests\",\n      \"128 requests\",\n      \"256 requests\"\n    ],\n    correct: 2,\n    explanation: \"With a row size of 8KB and cache block size of 64B, there are 128 (8KB/64B) requests of T0 (STREAM) serviced before T1 (RANDOM) gets a chance, demonstrating the unfairness of the row-hit first policy.\"\n  },\n  {\n    question: \"What is the main takeaway about abstraction layers from this lecture?\",\n    options: [\n      \"Abstraction layers should never be crossed\",\n      \"Breaking abstraction layers and knowing what is underneath enables problem solving\",\n      \"Only hardware designers need to understand multiple layers\",\n      \"Abstraction layers are only theoretical concepts\"\n    ],\n    correct: 1,\n    explanation: \"The main takeaway is that breaking the abstraction layers (between components and transformation hierarchy levels) and knowing what is underneath enables you to solve problems and design better future systems. Cooperation between multiple components and layers can enable more effective solutions.\"\n  }\n],\n'Chapter 5: What is A Computer and Von Neumann Model': [\n  {\n    question: \"What are the three key components that define a computer?\",\n    options: [\n      \"Hardware, software, and users\",\n      \"Computation, communication, and storage (memory)\",\n      \"Input, processing, and output\",\n      \"CPU, RAM, and hard drive\"\n    ],\n    correct: 1,\n    explanation: \"A computer is defined by three key components: Computation (processing), Communication (I/O), and Storage (memory). These components work together to form a complete computing system.\"\n  },\n  {\n    question: \"What are the two key properties of the Von Neumann model?\",\n    options: [\n      \"Fast processing and large memory\",\n      \"Stored program and sequential instruction processing\",\n      \"Multiple cores and parallel processing\",\n      \"Input/output capabilities and user interface\"\n    ],\n    correct: 1,\n    explanation: \"The Von Neumann model has two key properties: (1) Stored program - instructions stored in linear memory array with unified memory for instructions and data, (2) Sequential instruction processing - one instruction processed at a time with Program Counter identifying current instruction.\"\n  },\n  {\n    question: \"In the Von Neumann model, what determines whether a stored value is interpreted as an instruction?\",\n    options: [\n      \"The value itself\",\n      \"The memory location\",\n      \"The control signals\",\n      \"The data type\"\n    ],\n    correct: 2,\n    explanation: \"In the Von Neumann model, the interpretation of a stored value depends on the control signals. The same bit pattern can be interpreted as data or as an instruction depending on how the control unit processes it.\"\n  },\n  {\n    question: \"What is another name for the Von Neumann architecture?\",\n    options: [\n      \"Parallel processing computer\",\n      \"Stored program computer\",\n      \"Data flow computer\",\n      \"Multi-core computer\"\n    ],\n    correct: 1,\n    explanation: \"The Von Neumann architecture is also called a 'stored program computer' because instructions are stored in memory along with data, rather than being hardwired into the machine.\"\n  },\n  {\n    question: \"In the Von Neumann model, how is the Program Counter (instruction pointer) advanced?\",\n    options: [\n      \"Randomly based on available instructions\",\n      \"Based on data availability\",\n      \"Sequentially except for control transfer instructions\",\n      \"In parallel for multiple instructions\"\n    ],\n    correct: 2,\n    explanation: \"The Program Counter is advanced sequentially except for control transfer instructions (like jumps, branches, calls). This sequential advancement is a fundamental characteristic of Von Neumann execution.\"\n  },\n  {\n    question: \"In the dataflow model, when is an instruction executed?\",\n    options: [\n      \"When the instruction pointer points to it\",\n      \"When all its operands are ready\",\n      \"In sequential order\",\n      \"When the CPU is idle\"\n    ],\n    correct: 1,\n    explanation: \"In the dataflow model, an instruction is executed when all its operands are ready (i.e., when all inputs have tokens). There is no instruction pointer - execution is driven by data availability.\"\n  },\n  {\n    question: \"What is the main difference between Von Neumann and dataflow execution models?\",\n    options: [\n      \"Von Neumann uses more memory\",\n      \"Von Neumann is control-driven/sequential, dataflow is data-driven/parallel\",\n      \"Dataflow is slower than Von Neumann\",\n      \"Von Neumann requires special hardware\"\n    ],\n    correct: 1,\n    explanation: \"Von Neumann model is control-driven with sequential execution (instruction pointer controls order), while dataflow model is data-driven with potentially parallel execution (data availability controls order).\"\n  },\n  {\n    question: \"Which execution model is inherently more parallel?\",\n    options: [\n      \"Von Neumann model\",\n      \"Dataflow model\",\n      \"Both are equally parallel\",\n      \"Neither supports parallelism\"\n    ],\n    correct: 1,\n    explanation: \"The dataflow model is inherently more parallel because multiple instructions can 'fire' (execute) simultaneously when their operands are ready, unlike Von Neumann's sequential execution model.\"\n  },\n  {\n    question: \"In a dataflow machine, what causes a data flow node to 'fire'?\",\n    options: [\n      \"A clock signal\",\n      \"The instruction pointer\",\n      \"When all its inputs have tokens (are ready)\",\n      \"A random trigger\"\n    ],\n    correct: 2,\n    explanation: \"A data flow node fires (is fetched and executed) when all its inputs are ready, i.e., when all inputs have tokens. This is the fundamental execution principle of dataflow computing.\"\n  },\n  {\n    question: \"What major instruction set architectures use the Von Neumann model today?\",\n    options: [\n      \"Only x86\",\n      \"x86, ARM, MIPS, SPARC, Alpha, POWER\",\n      \"Only ARM and x86\",\n      \"Only older architectures\"\n    ],\n    correct: 1,\n    explanation: \"All major instruction set architectures today use the Von Neumann model, including x86, ARM, MIPS, SPARC, Alpha, and POWER architectures.\"\n  },\n  {\n    question: \"At the microarchitecture level, how do modern processors actually execute instructions?\",\n    options: [\n      \"Exactly as specified by Von Neumann model\",\n      \"Very differently from Von Neumann model (pipelined, out-of-order, etc.)\",\n      \"Only in sequential order\",\n      \"Without any optimization\"\n    ],\n    correct: 1,\n    explanation: \"Modern microarchitectures execute very differently from the Von Neumann model - using pipelined execution, multiple instructions at a time, out-of-order execution, and separate instruction/data caches, but this is not exposed to software.\"\n  },\n  {\n    question: \"What is the key difference between ISA and microarchitecture?\",\n    options: [\n      \"ISA is hardware, microarchitecture is software\",\n      \"ISA is the agreed interface between SW/HW, microarchitecture is the specific implementation\",\n      \"They are the same thing\",\n      \"ISA is old, microarchitecture is new\"\n    ],\n    correct: 1,\n    explanation: \"ISA is the agreed upon interface between software and hardware (what software writer needs to know), while microarchitecture is the specific implementation of an ISA (not visible to software).\"\n  },\n  {\n    question: \"Using the car analogy, what represents ISA vs. microarchitecture?\",\n    options: [\n      \"Engine vs. wheels\",\n      \"Gas pedal (interface) vs. engine internals (implementation)\",\n      \"Steering wheel vs. brakes\",\n      \"Exterior vs. interior\"\n    ],\n    correct: 1,\n    explanation: \"The gas pedal represents ISA (interface for 'acceleration' that driver uses), while the internals of the engine represent microarchitecture (how 'acceleration' is actually implemented).\"\n  },\n  {\n    question: \"Which changes faster: ISA or microarchitecture?\",\n    options: [\n      \"ISA changes faster\",\n      \"Microarchitecture changes faster\",\n      \"They change at the same rate\",\n      \"Neither changes\"\n    ],\n    correct: 1,\n    explanation: \"Microarchitecture usually changes faster than ISA. There are few ISAs (x86, ARM, SPARC, MIPS, Alpha) but many microarchitectures. For example, x86 ISA has many implementations: 286, 386, 486, Pentium, Pentium Pro, Pentium 4, Core, etc.\"\n  },\n  {\n    question: \"What does superscalar processing refer to?\",\n    options: [\n      \"Using multiple CPU cores\",\n      \"A technique to execute multiple instructions in parallel within the same processor core\",\n      \"Increasing clock frequency\",\n      \"Adding more memory\"\n    ],\n    correct: 1,\n    explanation: \"Superscalar processing is a technique used in modern microprocessor design to increase instruction throughput by executing multiple instructions in parallel within the same processor core, allowing more than one instruction per clock cycle.\"\n  },\n  {\n    question: \"Which of the following is part of the ISA?\",\n    options: [\n      \"Number of ports to the register file\",\n      \"ADD instruction's opcode\",\n      \"Whether machine employs pipelined execution\",\n      \"Number of cycles to execute MUL instruction\"\n    ],\n    correct: 1,\n    explanation: \"ADD instruction's opcode is part of the ISA as it defines the instruction interface. Number of register file ports, pipelining, and execution cycles are microarchitecture implementation details not visible to software.\"\n  },\n  {\n    question: \"Which of the following is part of microarchitecture?\",\n    options: [\n      \"Number of general purpose registers\",\n      \"Instruction opcodes\",\n      \"Number of ports to the register file\",\n      \"Memory addressing modes\"\n    ],\n    correct: 2,\n    explanation: \"Number of ports to the register file is a microarchitecture detail (implementation choice for performance). Number of registers, opcodes, and addressing modes are ISA specifications visible to programmers.\"\n  },\n  {\n    question: \"What does ISA specify regarding instructions?\",\n    options: [\n      \"Only the instruction format\",\n      \"Opcodes, addressing modes, data types, instruction types and formats, registers, condition codes\",\n      \"Only the execution time\",\n      \"Only the memory requirements\"\n    ],\n    correct: 1,\n    explanation: \"ISA specifies comprehensive instruction-related elements: opcodes, addressing modes, data types, instruction types and formats, registers, and condition codes - everything a programmer needs to know to write programs.\"\n  },\n  {\n    question: \"Which of the following are microarchitecture implementation choices?\",\n    options: [\n      \"Virtual memory management\",\n      \"Pipelining, out-of-order execution, caching policies, superscalar processing\",\n      \"Instruction set definition\",\n      \"Memory addressing modes\"\n    ],\n    correct: 1,\n    explanation: \"Microarchitecture includes implementation choices like pipelining, in-order vs out-of-order execution, memory access scheduling, superscalar processing, caching policies, prefetching, etc. - all done without exposure to software.\"\n  },\n  {\n    question: \"In the out-of-order execution example with instructions (1) mov eax,0 (2) mov edx,1 (3) mov edx,3 (4) inc edx (5) mov ecx,3, what determines the execution order?\",\n    options: [\n      \"The original program order must be maintained\",\n      \"The transistors/hardware decides based on dependencies and available resources\",\n      \"Random selection\",\n      \"Always execute in reverse order\"\n    ],\n    correct: 1,\n    explanation: \"In out-of-order execution, the transistors (hardware) decide which instructions to execute based on data dependencies and available execution resources, while maintaining the correct program semantics.\"\n  }\n],\n'Chapter 6: ISA Tradeoffs': [\n  {\n    question: \"What is a design point in computer architecture?\",\n    options: [\n      \"A specific location on the processor chip\",\n      \"A set of design considerations and their importance that leads to tradeoffs\",\n      \"The final stage of processor design\",\n      \"A testing methodology for processors\"\n    ],\n    correct: 1,\n    explanation: \"A design point is a set of design considerations and their importance that leads to tradeoffs in both ISA and microarchitecture. It's determined by the application space and intended users/market.\"\n  },\n  {\n    question: \"Which of the following are key design considerations mentioned in the lecture?\",\n    options: [\n      \"Only cost and performance\",\n      \"Cost, performance, power consumption, energy consumption, availability, reliability, time to market\",\n      \"Only performance and reliability\",\n      \"Hardware complexity and software compatibility\"\n    ],\n    correct: 1,\n    explanation: \"The key design considerations include: Cost, Performance, Maximum power consumption, Energy consumption (battery life), Availability, Reliability and Correctness, and Time to Market.\"\n  },\n  {\n    question: \"What determines the design point of a computer system?\",\n    options: [\n      \"The available technology\",\n      \"The manufacturing cost\",\n      \"The 'Problem' space (application space) and intended users/market\",\n      \"Government regulations\"\n    ],\n    correct: 2,\n    explanation: \"The design point is determined by the 'Problem' space (application space) and the intended users/market, which influences the relative importance of different design considerations.\"\n  },\n  {\n    question: \"What are the two main components of an instruction?\",\n    options: [\n      \"Address and data\",\n      \"Opcode and operands\",\n      \"Source and destination\",\n      \"Input and output\"\n    ],\n    correct: 1,\n    explanation: \"An instruction consists of: (1) opcode - what the instruction does, and (2) operands - who it is to do it to. This is the basic element of the HW/SW interface.\"\n  },\n  {\n    question: \"What is the concept of 'bit steering' in instruction encoding?\",\n    options: [\n      \"Using bits to control data flow direction\",\n      \"A bit in the instruction determines the interpretation of other bits\",\n      \"Steering bits toward the ALU\",\n      \"Managing bit-level operations\"\n    ],\n    correct: 1,\n    explanation: \"Bit steering is a concept where a bit in the instruction determines the interpretation of other bits, allowing for more efficient use of the instruction encoding space.\"\n  },\n  {\n    question: \"In a 0-address (stack) machine, how are operations performed?\",\n    options: [\n      \"Using registers only\",\n      \"Operations work on top elements of the stack (push/pop)\",\n      \"Direct memory addressing\",\n      \"Using accumulator register\"\n    ],\n    correct: 1,\n    explanation: \"In a 0-address stack machine, operations work on the top elements of the stack. Operands are pushed onto the stack, operations are performed on stack top elements, and results are popped off.\"\n  },\n  {\n    question: \"What characterizes a 1-address (accumulator) machine?\",\n    options: [\n      \"All operations use stack\",\n      \"Operations use accumulator register (op ACC, ld A, st A)\",\n      \"Two operands per instruction\",\n      \"Three separate operands\"\n    ],\n    correct: 1,\n    explanation: \"In a 1-address accumulator machine, operations typically involve the accumulator register (ACC). Instructions like 'op ACC', 'ld A' (load into ACC), 'st A' (store from ACC) are characteristic.\"\n  },\n  {\n    question: \"In a 2-address machine, what happens to one of the operands?\",\n    options: [\n      \"It remains unchanged\",\n      \"One operand is both source and destination (gets clobbered)\",\n      \"It gets copied to memory\",\n      \"It gets pushed to stack\"\n    ],\n    correct: 1,\n    explanation: \"In a 2-address machine (op S,D), one operand serves as both source and destination, meaning the original value gets overwritten (clobbered) with the operation result.\"\n  },\n  {\n    question: \"What is the main advantage of a 3-address machine?\",\n    options: [\n      \"Smaller instruction size\",\n      \"Source and destination are separate (op S1,S2,D)\",\n      \"Faster execution\",\n      \"Lower power consumption\"\n    ],\n    correct: 1,\n    explanation: \"In a 3-address machine (op S1,S2,D), the source operands and destination are separate, which means source values are preserved and not clobbered during operations.\"\n  },\n  {\n    question: \"What are the main advantages of stack machines?\",\n    options: [\n      \"Large instruction size and complex logic\",\n      \"Small instruction size, simpler logic, compact code, efficient procedure calls\",\n      \"High flexibility and parallel operations\",\n      \"Complex data type support\"\n    ],\n    correct: 1,\n    explanation: \"Stack machines have: small instruction size (no operands needed for operate instructions), simpler logic, compact code, and efficient procedure calls (all parameters on stack with no additional cycles for parameter passing).\"\n  },\n  {\n    question: \"What are the main disadvantages of stack machines?\",\n    options: [\n      \"Large code size and slow execution\",\n      \"Computations not easily expressible in postfix notation are difficult; limited flexibility\",\n      \"High power consumption\",\n      \"Complex instruction decoding\"\n    ],\n    correct: 1,\n    explanation: \"Stack machines have disadvantages: computations not easily expressible with postfix notation are difficult to map, cannot perform operations on many values simultaneously (only top N values), and lack flexibility.\"\n  },\n  {\n    question: \"The PDP-11 is an example of which type of machine?\",\n    options: [\n      \"0-address (stack) machine\",\n      \"1-address (accumulator) machine\",\n      \"2-address machine\",\n      \"3-address machine\"\n    ],\n    correct: 2,\n    explanation: \"The PDP-11 is a 2-address machine. Its ADD instruction has a 4-bit opcode and 2 6-bit operand specifiers, with limited bits to specify an instruction.\"\n  },\n  {\n    question: \"What is the main disadvantage of the PDP-11's 2-address design?\",\n    options: [\n      \"Too many operands\",\n      \"One source operand is always clobbered with the result\",\n      \"Instructions are too long\",\n      \"Cannot access memory\"\n    ],\n    correct: 1,\n    explanation: \"In PDP-11's 2-address design, one source operand is always clobbered (overwritten) with the result of the instruction, requiring additional steps to preserve original values when needed.\"\n  },\n  {\n    question: \"What type of machine is the Alpha architecture?\",\n    options: [\n      \"2-address memory/memory machine\",\n      \"Stack machine\",\n      \"3-address load/store machine\",\n      \"1-address accumulator machine\"\n    ],\n    correct: 2,\n    explanation: \"Alpha is a 3-address load/store machine, meaning it has separate source and destination operands, and memory access is only through explicit load and store instructions.\"\n  },\n  {\n    question: \"What type of machine is x86?\",\n    options: [\n      \"3-address load/store machine\",\n      \"2-address memory/memory machine\",\n      \"Stack machine only\",\n      \"1-address accumulator machine\"\n    ],\n    correct: 1,\n    explanation: \"x86 is a 2-address memory/memory machine, meaning it can perform operations directly between memory locations and registers, with one operand serving as both source and destination.\"\n  },\n  {\n    question: \"How is a data type defined in ISA context?\",\n    options: [\n      \"Any binary representation\",\n      \"Representation of information for which there are instructions that operate on the representation\",\n      \"Only primitive types like integers\",\n      \"Memory storage format only\"\n    ],\n    correct: 1,\n    explanation: \"A data type is defined as a representation of information for which there are instructions that operate on that representation. It's not just about storage format, but about having instruction support.\"\n  },\n  {\n    question: \"Which of the following are examples of data types mentioned?\",\n    options: [\n      \"Only integer and floating point\",\n      \"Integer, floating point, character, binary, decimal, BCD, doubly linked list, queue, string, bit vector, stack\",\n      \"Only primitive data types\",\n      \"Only numeric types\"\n    ],\n    correct: 1,\n    explanation: \"The lecture mentions various data types: Integer, floating point, character, binary, decimal, BCD, doubly linked list, queue, string, bit vector, and stack - ranging from primitive to complex structured types.\"\n  },\n  {\n    question: \"What is an example of a high-level data type instruction from VAX?\",\n    options: [\n      \"ADD and SUB only\",\n      \"INSQUEUE (Insert Queue) and REMQUEUE (Remove Queue) on doubly linked lists\",\n      \"Only load and store\",\n      \"Basic arithmetic operations\"\n    ],\n    correct: 1,\n    explanation: \"VAX provided high-level instructions like INSQUEUE (Insert Queue) and REMQUEUE (Remove Queue) that operated on doubly linked lists or queues, and FINDFIRST for complex data structure operations.\"\n  },\n  {\n    question: \"What does the 'semantic gap' refer to in computer architecture?\",\n    options: [\n      \"Physical distance between components\",\n      \"The disparity between high-level software concepts and low-level hardware operations\",\n      \"Time delay in instruction execution\",\n      \"Memory access latency\"\n    ],\n    correct: 1,\n    explanation: \"The semantic gap refers to the disparity between high-level concepts and abstractions used in software programming and the low-level operations and mechanisms implemented in hardware.\"\n  },\n  {\n    question: \"How do Early RISC architectures and Intel 432 differ in terms of semantic gap?\",\n    options: [\n      \"Both have the same approach\",\n      \"Early RISC: only integer data type (large gap); Intel 432: object data type, capability-based (small gap)\",\n      \"Both focus on complex data types\",\n      \"Both use only primitive types\"\n    ],\n    correct: 1,\n    explanation: \"Early RISC architectures had only integer data types (creating a large semantic gap), while Intel 432 supported object data types and was capability-based (attempting to close the semantic gap with high-level features).\"\n  }\n],\n  'Chapter 7: ISA Tradeoffs': [\n    // Data Types (Pages 3-5)\n    {\n      question: \"Which ISA introduced dedicated instructions for doubly linked list operations like INSQUEUE?\",\n      options: [\"x86\", \"VAX\", \"MIPS\", \"ARM\"],\n      correct: 1,\n      explanation: \"VAX had specialized instructions for queue/list operations, reflecting its CISC design philosophy.\"\n    },\n    {\n      question: \"What was the primary data type supported in early RISC architectures?\",\n      options: [\"Floating point\", \"Integer only\", \"Object references\", \"Binary-coded decimal\"],\n      correct: 1,\n      explanation: \"Early RISC designs like MIPS supported only integers to maintain simplicity.\"\n    },\n\n    // Semantic Gap (Pages 5,25)\n    {\n      question: \"The Intel 432's object data types illustrate what kind of semantic gap approach?\",\n      options: [\"Maximized gap\", \"Minimized gap\", \"No gap\", \"Variable gap\"],\n      correct: 1,\n      explanation: \"Intel 432 aimed to minimize the gap by supporting high-level constructs directly in hardware.\"\n    },\n\n    // Memory Organization (Pages 6-8)\n    {\n      question: \"In a 32-bit addressable system like the first Alpha, how would you add two 32-bit numbers?\",\n      options: [\n        \"Using two 32-bit load/store instructions\",\n        \"With a single 64-bit ADD instruction\",\n        \"Through memory-memory operations\",\n        \"Using bit-addressable operations\"\n      ],\n      correct: 0,\n      explanation: \"Required multiple operations due to 32-bit addressability constraints.\"\n    },\n    {\n      question: \"Which supercomputer architecture used 64-bit addressability?\",\n      options: [\"Burroughs 1700\", \"Cray-1\", \"Intel 432\", \"VAX-11\"],\n      correct: 1,\n      explanation: \"Cray supercomputers pioneered 64-bit addressability for scientific computing.\"\n    },\n\n    // Endianness (Pages 7-8)\n    {\n      question: \"Which architecture uses big-endian byte ordering?\",\n      options: [\"x86\", \"PowerPC\", \"ARM (little-endian mode)\", \"Original PCI bus\"],\n      correct: 1,\n      explanation: \"PowerPC and SPARC are notable big-endian architectures.\"\n    },\n    {\n      question: \"In little-endian systems, how is the value 0x12345678 stored at address A?\",\n      options: [\n        \"A:12 A+1:34 A+2:56 A+3:78\",\n        \"A:78 A+1:56 A+2:34 A+3:12\",\n        \"A:56 A+1:78 A+2:12 A+3:34\",\n        \"Split across cache lines\"\n      ],\n      correct: 1,\n      explanation: \"Little-endian stores least significant byte at lowest address.\"\n    },\n\n    // Registers (Pages 9-12)\n    {\n      question: \"What characteristic of programs justifies having registers in an ISA?\",\n      options: [\n        \"Spatial locality\",\n        \"Data locality (temporal and spatial)\",\n        \"Memory wall effect\",\n        \"Von Neumann bottleneck\"\n      ],\n      correct: 1,\n      explanation: \"Registers exploit temporal locality (reused data) and spatial locality (nearby data).\"\n    },\n    {\n      question: \"How many general-purpose registers did IA-64 (Itanium) introduce?\",\n      options: [\"8\", \"16\", \"32\", \"128\"],\n      correct: 3,\n      explanation: \"IA-64 expanded to 128 registers for explicit parallelism.\"\n    },\n\n    // Programmer Invisible State (Pages 10-11)\n    {\n      question: \"Why can't programmers directly access pipeline registers?\",\n      options: [\n        \"They are protected by the OS\",\n        \"They represent microarchitectural state\",\n        \"They are physically inaccessible\",\n        \"They violate memory protection\"\n      ],\n      correct: 1,\n      explanation: \"Pipeline registers are part of implementation-specific microarchitecture.\"\n    },\n\n    // Instruction Classes (Page 13)\n    {\n      question: \"Which instruction class changes the sequence of execution?\",\n      options: [\n        \"Operate instructions\",\n        \"Data movement instructions\",\n        \"Control flow instructions\",\n        \"Floating-point instructions\"\n      ],\n      correct: 2,\n      explanation: \"Control flow instructions (branches/jumps) alter the PC.\"\n    },\n\n    // Addressing Modes (Pages 14-19)\n    {\n      question: \"Which addressing mode combines a base register and index register?\",\n      options: [\n        \"Displacement\",\n        \"Register indirect\",\n        \"Indexed addressing\",\n        \"Memory indirect\"\n      ],\n      correct: 2,\n      explanation: \"Indexed addressing uses base + index calculation.\"\n    },\n    {\n      question: \"What does 'orthogonal ISA' mean?\",\n      options: [\n        \"Instructions use only right angles\",\n        \"All instructions can use all addressing modes\",\n        \"Fixed 90-degree instruction alignment\",\n        \"Separate integer/floating-point pipelines\"\n      ],\n      correct: 1,\n      explanation: \"Orthogonality means uniform combination of operations and addressing modes.\"\n    },\n\n    // Instruction Formats (Pages 20-22,28-29)\n    {\n      question: \"What is a key advantage of variable-length instructions?\",\n      options: [\n        \"Simpler hardware decoding\",\n        \"Better code density\",\n        \"Faster clock speeds\",\n        \"More registers\"\n      ],\n      correct: 1,\n      explanation: \"Variable-length enables compact encoding (e.g., x86).\"\n    },\n    {\n      question: \"Which field in MIPS I-type instructions holds the immediate value?\",\n      options: [\"rs\", \"rt\", \"opcode\", \"16-bit immediate field\"],\n      correct: 3,\n      explanation: \"I-type uses 16-bit immediate for constants/offsets.\"\n    },\n\n    // Complex vs Simple Instructions (Pages 23-26)\n    {\n      question: \"What was a major disadvantage of complex CISC instructions?\",\n      options: [\n        \"Limited compiler optimization opportunities\",\n        \"Too many registers\",\n        \"Fixed-length encoding\",\n        \"Lack of virtual memory support\"\n      ],\n      correct: 0,\n      explanation: \"Complex instructions created coarse-grained operations that constrained optimizations.\"\n    },\n    {\n      question: \"Which VAX instruction provided array access with bounds checking?\",\n      options: [\"MOV\", \"INDEX\", \"BOUNDS\", \"ARRAY\"],\n      correct: 1,\n      explanation: \"VAX INDEX instruction exemplified high-level language support.\"\n    },\n\n    // RISC vs CISC (Pages 26-27)\n    {\n      question: \"Which characteristic is NOT typical of RISC designs?\",\n      options: [\n        \"Many addressing modes\",\n        \"Uniform decode\",\n        \"Fixed-length instructions\",\n        \"Load/store architecture\"\n      ],\n      correct: 0,\n      explanation: \"RISC minimizes addressing modes for simplicity.\"\n    },\n    {\n      question: \"What motivated the x86's instruction prefixes?\",\n      options: [\n        \"Backward compatibility\",\n        \"Faster decoding\",\n        \"Fewer registers\",\n        \"Big-endian support\"\n      ],\n      correct: 0,\n      explanation: \"Prefixes allowed extending the ISA while maintaining compatibility.\"\n    },\n\n    // Evolution (Page 27)\n    {\n      question: \"What drove ISA evolution according to the chapter?\",\n      options: [\n        \"Compiler limitations\",\n        \"Memory constraints\",\n        \"Specialization needs\",\n        \"All of the above\"\n      ],\n      correct: 3,\n      explanation: \"All these factors historically influenced ISA design.\"\n    },\n\n    // Instruction Formats Deep Dive (Pages 28-29)\n    {\n      question: \"How many bytes can x86 instruction prefixes occupy?\",\n      options: [\"0\", \"1\", \"Up to 4\", \"Exactly 2\"],\n      correct: 2,\n      explanation: \"x86 allows up to four 1-byte prefixes.\"\n    },\n    {\n      question: \"What MIPS instruction format has a 26-bit immediate?\",\n      options: [\"R-type\", \"I-type\", \"J-type\", \"U-type\"],\n      correct: 2,\n      explanation: \"J-type (jump) uses 26-bit immediates for address targets.\"\n    }\n  ],\n  'Chapter 8: Single-Cycle Microarchitecture': [\n{\nquestion: \"What is the primary purpose of cache memory?\",\noptions: [\n\"To permanently store all program instructions\",\n\"To store active/commonly used instructions and speed up processing\",\n\"To replace the main memory entirely\",\n\"To store only the results of arithmetic operations\"\n],\ncorrect: 1,\nexplanation: \"Cache memory temporarily stores active or frequently used instructions to speed up processing and reduce bottlenecks between RAM and the CPU.\"\n},\n{\nquestion: \"During the fetch part of the instruction cycle, what is the role of the address bus?\",\noptions: [\n\"It carries the instruction's opcode to the ALU\",\n\"It carries the address of the instruction to main memory\",\n\"It stores the result of the executed instruction\",\n\"It decodes the instruction for the control unit\"\n],\ncorrect: 1,\nexplanation: \"The address bus carries the memory address of the instruction to be fetched from main memory to the CPU.\"\n},\n{\nquestion: \"Which of the following is a drawback of higher clock speeds in CPUs?\",\noptions: [\n\"Fewer instructions can be executed per second\",\n\"The CPU may overheat and require more cooling\",\n\"Programs run slower due to increased complexity\",\n\"The data bus width is reduced\"\n],\ncorrect: 1,\nexplanation: \"Higher clock speeds increase power consumption and heat generation, potentially causing overheating and requiring additional cooling solutions.\"\n},\n{\nquestion: \"In a single-cycle microarchitecture, how many clock cycles does each instruction take to execute?\",\noptions: [\n\"One cycle\",\n\"Two cycles\",\n\"Variable cycles depending on the instruction\",\n\"Six cycles (one per phase)\"\n],\ncorrect: 0,\nexplanation: \"In a single-cycle microarchitecture, all phases of an instruction (fetch, decode, execute, etc.) are completed within a single clock cycle.\"\n},\n{\nquestion: \"What determines the clock cycle time in a single-cycle microarchitecture?\",\noptions: [\n\"The fastest instruction\",\n\"The average instruction latency\",\n\"The slowest instruction\",\n\"The number of functional units\"\n],\ncorrect: 2,\nexplanation: \"The slowest instruction dictates the clock cycle time because all instructions must complete within one cycle.\"\n} \n],  \n'Chapter 9: Multi-Cycle and Pipelined Microarchitecture' : [\n{\nquestion: \"How does a multi-cycle microarchitecture differ from a single-cycle design?\",\noptions: [\n\"Instructions always take fewer cycles to complete\",\n\"Each phase of the instruction cycle may span multiple clock cycles\",\n\"It eliminates the need for a control unit\",\n\"It uses a smaller data bus\"\n],\ncorrect: 1,\nexplanation: \"In a multi-cycle microarchitecture, each phase (fetch, decode, etc.) can take multiple clock cycles, allowing for shorter cycle times.\"\n},\n{\nquestion: \"What is a key advantage of multi-cycle machines over single-cycle machines?\",\noptions: [\n\"The slowest instruction determines the cycle time\",\n\"Clock cycle time is shorter as it depends on the slowest stage, not instruction\",\n\"They require no control signals\",\n\"All instructions execute in parallel\"\n],\ncorrect: 1,\nexplanation: \"Multi-cycle machines break instructions into stages, allowing the clock cycle time to be determined by the slowest stage rather than the slowest instruction.\"\n},\n{\nquestion: \"Which component generates control signals to coordinate the datapath in instruction processing?\",\noptions: [\n\"ALU\",\n\"Cache memory\",\n\"Control logic\",\n\"Address bus\"\n],\ncorrect: 2,\nexplanation: \"The control logic decodes instructions and generates signals to direct the datapath (e.g., ALU, registers) on how to process data.\"\n},\n{\nquestion: \"What happens to the program counter (PC) during the fetch stage?\",\noptions: [\n\"It is reset to zero\",\n\"It is incremented to point to the next instruction\",\n\"It stores the result of the ALU operation\",\n\"It holds the opcode for decoding\"\n],\ncorrect: 1,\nexplanation: \"After fetching an instruction, the PC is incremented to point to the next instruction in memory.\"\n},\n{\nquestion: \"Which of the following is true about the execute stage?\",\noptions: [\n\"It retrieves the instruction from main memory\",\n\"It decodes the opcode into control signals\",\n\"It performs the actual operation (e.g., ALU computation)\",\n\"It stores the result in the instruction register\"\n],\ncorrect: 2,\nexplanation: \"The execute stage carries out the operation specified by the instruction, such as arithmetic in the ALU or data transfer.\"\n}\n],\n'Chapter 10: Introduction to Microarchitecture' : [\n{\nquestion: \"What is the key characteristic of a single-cycle microarchitecture?\",\noptions: [\n\"All instructions complete execution in one clock cycle\",\n\"Instructions are pipelined across multiple cycles\",\n\"Only arithmetic instructions use a single cycle\",\n\"Memory access takes variable cycles depending on latency\"\n],\ncorrect: 0,\nexplanation: \"In a single-cycle microarchitecture, every instruction (e.g., ALU, load/store) completes all stages (fetch, decode, execute, etc.) within one clock cycle.\"\n},\n\n{\nquestion: \"Which component is responsible for holding the current instruction address in the single-cycle datapath?\",\noptions: [\n\"Register file\",\n\"Program counter (PC)\",\n\"ALU\",\n\"Data memory\"\n],\ncorrect: 1,\nexplanation: \"The program counter (PC) stores the address of the current instruction being executed and increments by 4 (for MIPS) after each fetch.\"\n},\n\n{\nquestion: \"What is the role of the 'sign-extend' unit in the datapath?\",\noptions: [\n\"To convert 16-bit immediate values to 32-bit signed values\",\n\"To perform arithmetic operations\",\n\"To select between register or immediate operands\",\n\"To manage memory addresses\"\n],\ncorrect: 0,\nexplanation: \"The sign-extend unit expands 16-bit immediate values (e.g., in I-type instructions) to 32 bits while preserving the sign for correct arithmetic operations.\"\n},\n\n{\nquestion: \"Which control signal determines whether the ALU uses a register value or an immediate value as its second operand?\",\noptions: [\n\"RegWrite\",\n\"ALUSrc\",\n\"MemtoReg\",\n\"Branch\"\n],\ncorrect: 1,\nexplanation: \"ALUSrc selects between the second register operand (e.g., for R-type) or a sign-extended immediate (e.g., for ADDI/LW/SW).\"\n},\n\n{\nquestion: \"What happens during the 'MEM' stage of the LW instruction?\",\noptions: [\n\"The ALU computes the effective memory address\",\n\"Data is read from memory and written to a register\",\n\"The instruction is fetched from memory\",\n\"The PC is updated\"\n],\ncorrect: 1,\nexplanation: \"In the MEM stage, the data memory is accessed using the address computed in EX, and the result is later written back to a register in WB.\"\n},\n\n{\nquestion: \"Which instruction type requires the 'RegDst' control signal to select the destination register from bits [15:11]?\",\noptions: [\n\"I-type (e.g., ADDI)\",\n\"R-type (e.g., ADD)\",\n\"Load (e.g., LW)\",\n\"Store (e.g., SW)\"\n],\ncorrect: 1,\nexplanation: \"R-type instructions use bits [15:11] for the destination register (rd), while I-type (e.g., ADDI/LW) use bits [20:16] (rt).\"\n},\n\n{\nquestion: \"Why is the single-cycle design inefficient for real-world implementations?\",\noptions: [\n\"It cannot handle branch instructions\",\n\"The clock cycle must accommodate the slowest instruction (e.g., LW)\",\n\"It lacks a register file\",\n\"Memory accesses are asynchronous\"\n],\ncorrect: 1,\nexplanation: \"The clock cycle length is determined by the slowest instruction (e.g., LW, which uses memory access), making faster instructions (e.g., ADD) unnecessarily slow.\"\n},\n\n{\nquestion: \"What is the purpose of the 'MemtoReg' multiplexer in the datapath?\",\noptions: [\n\"To select between ALU result or memory data for register writeback\",\n\"To choose between register operands\",\n\"To extend immediate values\",\n\"To compute branch targets\"\n],\ncorrect: 0,\nexplanation: \"MemtoReg selects whether the writeback data comes from the ALU result (e.g., for ADD) or memory (e.g., for LW).\"\n},\n],\n\n};\n\n\n// Arabic translation of quiz data\nconst quizDataArabic = {\n};\n\n// Create a mapping between English and Arabic chapter names\nconst chapterMapping = {\n  'Chapter 1: Introduction': 'الفصل 1: مقدمة',\n  'Chapter 2: Storage Environment and RAID': 'الفصل 2: ',\n  'Chapter 3: Computer Architecture Introduction and Basics': 'الفصل 3: ',\n  'Chapter 4: Introduction and Basics': 'الفصل 4: ',\n  'Chapter 5: What is A Computer and Von Neumann Model': 'الفصل 5: ',\n  'Chapter 6: Introduction to ISA Tradeoffs': 'الفصل 6: ',\n  'Chapter 7: ISA Tradeoffs afterlecture': 'الفصل 7: ',\n  'Chapter 8: Single-Cycle Microarchitecture': 'الفصل 8: ',\n  'Chapter 9: Multi-Cycle and Pipelined Microarchitecture': 'الفصل 9: ',\n  'Chapter 10: Introduction to Microarchitecture': 'الفصل 10: '\n};\n\n// Reverse mapping (Arabic to English)\nconst reverseChapterMapping = {};\nObject.keys(chapterMapping).forEach(englishName => {\n  reverseChapterMapping[chapterMapping[englishName]] = englishName;\n});\n\nexport { quizData, quizDataArabic, chapterMapping, reverseChapterMapping };\n"],"mappings":"AAAA,KAAM,CAAAA,QAAQ,CAAG,CACjB,yBAAyB,CAAE,CACvB,CACEC,QAAQ,CAAE,uFAAuF,CACjGC,OAAO,CAAE,CACP,oEAAoE,CACpE,8GAA8G,CAC9G,+DAA+D,CAC/D,sCAAsC,CACvC,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,8MACf,CAAC,CACD,CACEH,QAAQ,CAAE,yEAAyE,CACnFC,OAAO,CAAE,CACP,iBAAiB,CACjB,cAAc,CACd,kBAAkB,CAClB,eAAe,CAChB,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,iKACf,CAAC,CACD,CACEH,QAAQ,CAAE,sDAAsD,CAChEC,OAAO,CAAE,CACP,wCAAwC,CACxC,wDAAwD,CACxD,6CAA6C,CAC7C,mCAAmC,CACpC,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,uGACf,CAAC,CACD,CACEH,QAAQ,CAAE,0CAA0C,CACpDC,OAAO,CAAE,CACP,qBAAqB,CACrB,mDAAmD,CACnD,sCAAsC,CACtC,4BAA4B,CAC7B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,wIACf,CAAC,CACD,CACEH,QAAQ,CAAE,wEAAwE,CAClFC,OAAO,CAAE,CACP,yBAAyB,CACzB,wCAAwC,CACxC,wCAAwC,CACxC,uCAAuC,CACxC,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,mHACf,CAAC,CACD,CACEH,QAAQ,CAAE,gFAAgF,CAC1FC,OAAO,CAAE,CACP,6BAA6B,CAC7B,QAAQ,CACR,cAAc,CACd,WAAW,CACZ,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,yIACf,CAAC,CACD,CACEH,QAAQ,CAAE,8CAA8C,CACxDC,OAAO,CAAE,CACP,+BAA+B,CAC/B,uBAAuB,CACvB,6DAA6D,CAC7D,oCAAoC,CACrC,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,oKACf,CAAC,CACD,CACEH,QAAQ,CAAE,sEAAsE,CAChFC,OAAO,CAAE,CACP,sBAAsB,CACtB,2BAA2B,CAC3B,+BAA+B,CAC/B,kBAAkB,CACnB,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,qFACf,CAAC,CACD,CACEH,QAAQ,CAAE,4DAA4D,CACtEC,OAAO,CAAE,CACP,yCAAyC,CACzC,0CAA0C,CAC1C,mEAAmE,CACnE,qCAAqC,CACtC,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,wHACf,CAAC,CACD,CACEH,QAAQ,CAAE,oEAAoE,CAC9EC,OAAO,CAAE,CACP,oCAAoC,CACpC,wCAAwC,CACxC,wEAAwE,CACxE,kCAAkC,CACnC,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,kKACf,CAAC,CACD,CACEH,QAAQ,CAAE,6DAA6D,CACvEC,OAAO,CAAE,CACP,2BAA2B,CAC3B,+CAA+C,CAC/C,qCAAqC,CACrC,qCAAqC,CACtC,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,qNACf,CAAC,CACD,CACEH,QAAQ,CAAE,8DAA8D,CACxEC,OAAO,CAAE,CACP,0BAA0B,CAC1B,gDAAgD,CAChD,uEAAuE,CACvE,oDAAoD,CACrD,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,wMACf,CAAC,CACD,CACEH,QAAQ,CAAE,qDAAqD,CAC/DC,OAAO,CAAE,CACP,qCAAqC,CACrC,2DAA2D,CAC3D,uCAAuC,CACvC,6BAA6B,CAC9B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,iKACf,CAAC,CACD,CACEH,QAAQ,CAAE,sFAAsF,CAChGC,OAAO,CAAE,CACP,6BAA6B,CAC7B,6BAA6B,CAC7B,qBAAqB,CACrB,4BAA4B,CAC7B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,2KACf,CAAC,CACD,CACEH,QAAQ,CAAE,wEAAwE,CAClFC,OAAO,CAAE,CACP,oCAAoC,CACpC,4DAA4D,CAC5D,0CAA0C,CAC1C,8CAA8C,CAC/C,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,yHACf,CAAC,CACD,CACEH,QAAQ,CAAE,mFAAmF,CAC7FC,OAAO,CAAE,CACP,wBAAwB,CACxB,0BAA0B,CAC1B,mFAAmF,CACnF,uBAAuB,CACxB,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,qMACf,CAAC,CACD,CACEH,QAAQ,CAAE,yGAAyG,CACnHC,OAAO,CAAE,CACP,0BAA0B,CAC1B,oBAAoB,CACpB,sFAAsF,CACtF,2BAA2B,CAC5B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,wMACf,CAAC,CACD,CACEH,QAAQ,CAAE,2EAA2E,CACrFC,OAAO,CAAE,CACP,+CAA+C,CAC/C,yFAAyF,CACzF,mCAAmC,CACnC,0CAA0C,CAC3C,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,uKACf,CAAC,CACD,CACEH,QAAQ,CAAE,sEAAsE,CAChFC,OAAO,CAAE,CACP,qCAAqC,CACrC,2BAA2B,CAC3B,sDAAsD,CACtD,yBAAyB,CAC1B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,2LACf,CAAC,CACD,CACEH,QAAQ,CAAE,uEAAuE,CACjFC,OAAO,CAAE,CACP,gBAAgB,CAChB,iBAAiB,CACjB,oBAAoB,CACpB,0BAA0B,CAC3B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,wLACf,CAAC,CACD,CACEH,QAAQ,CAAE,oDAAoD,CAC9DC,OAAO,CAAE,CACP,8BAA8B,CAC9B,mDAAmD,CACnD,wDAAwD,CACxD,qDAAqD,CACtD,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,4NACf,CAAC,CACD,CACEH,QAAQ,CAAE,0EAA0E,CACpFC,OAAO,CAAE,CACP,gDAAgD,CAChD,qDAAqD,CACrD,+BAA+B,CAC/B,kDAAkD,CACnD,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,6MACf,CAAC,CACF,CACD,yCAAyC,CAAE,CAC3C,CACEH,QAAQ,CAAE,yEAAyE,CACnFC,OAAO,CAAE,CACP,iDAAiD,CACjD,4FAA4F,CAC5F,gDAAgD,CAChD,oEAAoE,CACrE,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,oMACf,CAAC,CACD,CACEH,QAAQ,CAAE,sEAAsE,CAChFC,OAAO,CAAE,CACP,wBAAwB,CACxB,4BAA4B,CAC5B,mBAAmB,CACnB,gBAAgB,CACjB,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,+JACf,CAAC,CACD,CACEH,QAAQ,CAAE,6DAA6D,CACvEC,OAAO,CAAE,CACP,wDAAwD,CACxD,6DAA6D,CAC7D,4EAA4E,CAC5E,oDAAoD,CACrD,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,4JACf,CAAC,CACD,CACEH,QAAQ,CAAE,oEAAoE,CAC9EC,OAAO,CAAE,CACP,qCAAqC,CACrC,mCAAmC,CACnC,8BAA8B,CAC9B,+BAA+B,CAChC,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,8FACf,CAAC,CACD,CACEH,QAAQ,CAAE,0CAA0C,CACpDC,OAAO,CAAE,CACP,kBAAkB,CAClB,WAAW,CACX,sFAAsF,CACtF,4BAA4B,CAC7B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,qGACf,CAAC,CACD,CACEH,QAAQ,CAAE,iEAAiE,CAC3EC,OAAO,CAAE,CACP,sDAAsD,CACtD,6CAA6C,CAC7C,yFAAyF,CACzF,oDAAoD,CACrD,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,yHACf,CAAC,CACD,CACEH,QAAQ,CAAE,2DAA2D,CACrEC,OAAO,CAAE,CACP,KAAK,CACL,KAAK,CACL,KAAK,CACL,MAAM,CACP,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,2IACf,CAAC,CACD,CACEH,QAAQ,CAAE,0CAA0C,CACpDC,OAAO,CAAE,CACP,iBAAiB,CACjB,eAAe,CACf,kBAAkB,CAClB,mBAAmB,CACpB,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,uDACf,CAAC,CACD,CACEH,QAAQ,CAAE,6DAA6D,CACvEC,OAAO,CAAE,CACP,mBAAmB,CACnB,UAAU,CACV,MAAM,CACN,YAAY,CACb,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,kHACf,CAAC,CACD,CACEH,QAAQ,CAAE,4DAA4D,CACtEC,OAAO,CAAE,CACP,kDAAkD,CAClD,mDAAmD,CACnD,+CAA+C,CAC/C,2DAA2D,CAC5D,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,qIACf,CAAC,CACD,CACEH,QAAQ,CAAE,kDAAkD,CAC5DC,OAAO,CAAE,CACP,yBAAyB,CACzB,8BAA8B,CAC9B,wBAAwB,CACxB,4BAA4B,CAC7B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,+IACf,CAAC,CACD,CACEH,QAAQ,CAAE,2BAA2B,CACrCC,OAAO,CAAE,CACP,mCAAmC,CACnC,sCAAsC,CACtC,oCAAoC,CACpC,gCAAgC,CACjC,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,uDACf,CAAC,CACD,CACEH,QAAQ,CAAE,sFAAsF,CAChGC,OAAO,CAAE,CACP,WAAW,CACX,UAAU,CACV,oBAAoB,CACpB,iBAAiB,CAClB,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,gFACf,CAAC,CACD,CACEH,QAAQ,CAAE,kEAAkE,CAC5EC,OAAO,CAAE,CACP,QAAQ,CACR,QAAQ,CACR,QAAQ,CACR,SAAS,CACV,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,0FACf,CAAC,CACD,CACEH,QAAQ,CAAE,0DAA0D,CACpEC,OAAO,CAAE,CACP,QAAQ,CACR,SAAS,CACT,SAAS,CACT,SAAS,CACV,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,gGACf,CAAC,CACD,CACEH,QAAQ,CAAE,uDAAuD,CACjEC,OAAO,CAAE,CACP,UAAU,CACV,QAAQ,CACR,SAAS,CACT,QAAQ,CACT,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,+EACf,CAAC,CACD,CACEH,QAAQ,CAAE,0DAA0D,CACpEC,OAAO,CAAE,CACP,SAAS,CACT,SAAS,CACT,SAAS,CACT,SAAS,CACV,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,6FACf,CAAC,CACD,CACEH,QAAQ,CAAE,4CAA4C,CACtDC,OAAO,CAAE,CACP,QAAQ,CACR,QAAQ,CACR,QAAQ,CACR,QAAQ,CACT,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,0EACf,CAAC,CACD,CACEH,QAAQ,CAAE,0CAA0C,CACpDC,OAAO,CAAE,CACP,uCAAuC,CACvC,kDAAkD,CAClD,gCAAgC,CAChC,qBAAqB,CACtB,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,sFACf,CAAC,CACD,CACEH,QAAQ,CAAE,sEAAsE,CAChFC,OAAO,CAAE,CACP,QAAQ,CACR,QAAQ,CACR,QAAQ,CACR,SAAS,CACV,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,qLACf,CAAC,CACD,CACEH,QAAQ,CAAE,mFAAmF,CAC7FC,OAAO,CAAE,CACP,QAAQ,CACR,QAAQ,CACR,QAAQ,CACR,SAAS,CACV,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,6KACf,CAAC,CACD,CACEH,QAAQ,CAAE,mFAAmF,CAC7FC,OAAO,CAAE,CACP,KAAK,CACL,KAAK,CACL,KAAK,CACL,MAAM,CACP,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,iHACf,CAAC,CACD,CACEH,QAAQ,CAAE,wGAAwG,CAClHC,OAAO,CAAE,CACP,KAAK,CACL,KAAK,CACL,KAAK,CACL,MAAM,CACP,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,gIACf,CAAC,CACD,CACEH,QAAQ,CAAE,4EAA4E,CACtFC,OAAO,CAAE,CACP,KAAK,CACL,KAAK,CACL,KAAK,CACL,MAAM,CACP,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,mHACf,CAAC,CACD,CACEH,QAAQ,CAAE,6EAA6E,CACvFC,OAAO,CAAE,CACP,eAAe,CACf,eAAe,CACf,eAAe,CACf,eAAe,CAChB,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,+HACf,CAAC,CACD,CACEH,QAAQ,CAAE,qCAAqC,CAC/CC,OAAO,CAAE,CACP,8DAA8D,CAC9D,oEAAoE,CACpE,qDAAqD,CACrD,4CAA4C,CAC7C,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,6LACf,CAAC,CACD,CACEH,QAAQ,CAAE,4CAA4C,CACtDC,OAAO,CAAE,CACP,wBAAwB,CACxB,0BAA0B,CAC1B,wBAAwB,CACxB,+BAA+B,CAChC,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,+HACf,CAAC,CACD,CACEH,QAAQ,CAAE,2CAA2C,CACrDC,OAAO,CAAE,CACP,wBAAwB,CACxB,mCAAmC,CACnC,2CAA2C,CAC3C,8CAA8C,CAC/C,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,iHACf,CAAC,CACD,CACEH,QAAQ,CAAE,qDAAqD,CAC/DC,OAAO,CAAE,CACP,iCAAiC,CACjC,qBAAqB,CACrB,gBAAgB,CAChB,4BAA4B,CAC7B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,0HACf,CAAC,CACD,CACEH,QAAQ,CAAE,2DAA2D,CACrEC,OAAO,CAAE,CACP,QAAQ,CACR,QAAQ,CACR,QAAQ,CACR,SAAS,CACV,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,sIACf,CAAC,CACF,CACD,0DAA0D,CAAE,CAC1D,CACEH,QAAQ,CAAE,qEAAqE,CAC/EC,OAAO,CAAE,CACP,8BAA8B,CAC9B,sCAAsC,CACtC,qCAAqC,CACrC,iCAAiC,CAClC,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,iLACf,CAAC,CACD,CACEH,QAAQ,CAAE,sEAAsE,CAChFC,OAAO,CAAE,CACP,UAAU,CACV,mBAAmB,CACnB,oBAAoB,CACpB,gBAAgB,CACjB,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,gHACf,CAAC,CACD,CACEH,QAAQ,CAAE,yDAAyD,CACnEC,OAAO,CAAE,CACP,oDAAoD,CACpD,8FAA8F,CAC9F,mCAAmC,CACnC,4CAA4C,CAC7C,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,qIACf,CAAC,CACD,CACEH,QAAQ,CAAE,iFAAiF,CAC3FC,OAAO,CAAE,CACP,4BAA4B,CAC5B,yEAAyE,CACzE,gDAAgD,CAChD,0CAA0C,CAC3C,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,0JACf,CAAC,CACD,CACEH,QAAQ,CAAE,2EAA2E,CACrFC,OAAO,CAAE,CACP,uCAAuC,CACvC,4EAA4E,CAC5E,4CAA4C,CAC5C,sCAAsC,CACvC,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,6NACf,CAAC,CACD,CACEH,QAAQ,CAAE,2GAA2G,CACrHC,OAAO,CAAE,CACP,yBAAyB,CACzB,kBAAkB,CAClB,uCAAuC,CACvC,iBAAiB,CAClB,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,gKACf,CAAC,CACD,CACEH,QAAQ,CAAE,gFAAgF,CAC1FC,OAAO,CAAE,CACP,oBAAoB,CACpB,kBAAkB,CAClB,kCAAkC,CAClC,6BAA6B,CAC9B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,gFACf,CAAC,CACD,CACEH,QAAQ,CAAE,4DAA4D,CACtEC,OAAO,CAAE,CACP,sCAAsC,CACtC,uCAAuC,CACvC,uCAAuC,CACvC,qCAAqC,CACtC,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,gIACf,CAAC,CACD,CACEH,QAAQ,CAAE,2EAA2E,CACrFC,OAAO,CAAE,CACP,4BAA4B,CAC5B,4CAA4C,CAC5C,wCAAwC,CACxC,oCAAoC,CACrC,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,kIACf,CAAC,CACD,CACEH,QAAQ,CAAE,0EAA0E,CACpFC,OAAO,CAAE,CACP,oCAAoC,CACpC,mHAAmH,CACnH,2CAA2C,CAC3C,8CAA8C,CAC/C,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,+KACf,CAAC,CACD,CACEH,QAAQ,CAAE,mCAAmC,CAC7CC,OAAO,CAAE,CACP,gCAAgC,CAChC,sCAAsC,CACtC,qBAAqB,CACrB,sBAAsB,CACvB,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,mHACf,CAAC,CACD,CACEH,QAAQ,CAAE,kDAAkD,CAC5DC,OAAO,CAAE,CACP,8DAA8D,CAC9D,8CAA8C,CAC9C,gDAAgD,CAChD,2BAA2B,CAC5B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,2HACf,CAAC,CACD,CACEH,QAAQ,CAAE,8CAA8C,CACxDC,OAAO,CAAE,CACP,iBAAiB,CACjB,iBAAiB,CACjB,YAAY,CACZ,gBAAgB,CACjB,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,6FACf,CAAC,CACD,CACEH,QAAQ,CAAE,mEAAmE,CAC7EC,OAAO,CAAE,CACP,yBAAyB,CACzB,sFAAsF,CACtF,8BAA8B,CAC9B,2BAA2B,CAC5B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,wMACf,CAAC,CACD,CACEH,QAAQ,CAAE,qFAAqF,CAC/FC,OAAO,CAAE,CACP,OAAO,CACP,OAAO,CACP,OAAO,CACP,OAAO,CACR,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,2FACf,CAAC,CACD,CACEH,QAAQ,CAAE,wDAAwD,CAClEC,OAAO,CAAE,CACP,qCAAqC,CACrC,qEAAqE,CACrE,6BAA6B,CAC7B,uCAAuC,CACxC,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,+JACf,CAAC,CACD,CACEH,QAAQ,CAAE,sDAAsD,CAChEC,OAAO,CAAE,CACP,8BAA8B,CAC9B,gCAAgC,CAChC,gCAAgC,CAChC,+BAA+B,CAChC,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,kLACf,CAAC,CACD,CACEH,QAAQ,CAAE,+DAA+D,CACzEC,OAAO,CAAE,CACP,QAAQ,CACR,QAAQ,CACR,QAAQ,CACR,OAAO,CACR,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,gGACf,CAAC,CACD,CACEH,QAAQ,CAAE,iDAAiD,CAC3DC,OAAO,CAAE,CACP,KAAK,CACL,KAAK,CACL,MAAM,CACN,MAAM,CACP,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,+GACf,CAAC,CACD,CACEH,QAAQ,CAAE,qGAAqG,CAC/GC,OAAO,CAAE,CACP,uBAAuB,CACvB,gDAAgD,CAChD,sEAAsE,CACtE,0BAA0B,CAC3B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,yKACf,CAAC,CACD,CACEH,QAAQ,CAAE,uEAAuE,CACjFC,OAAO,CAAE,CACP,2CAA2C,CAC3C,4GAA4G,CAC5G,4DAA4D,CAC5D,gEAAgE,CACjE,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,2JACf,CAAC,CACD,CACEH,QAAQ,CAAE,mEAAmE,CAC7EC,OAAO,CAAE,CACP,UAAU,CACV,qCAAqC,CACrC,eAAe,CACf,qBAAqB,CACtB,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,oJACf,CAAC,CACD,CACEH,QAAQ,CAAE,yEAAyE,CACnFC,OAAO,CAAE,CACP,4BAA4B,CAC5B,gFAAgF,CAChF,wBAAwB,CACxB,uBAAuB,CACxB,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,kLACf,CAAC,CACD,CACEH,QAAQ,CAAE,2DAA2D,CACrEC,OAAO,CAAE,CACP,KAAK,CACL,KAAK,CACL,MAAM,CACN,MAAM,CACP,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,kHACf,CAAC,CACD,CACEH,QAAQ,CAAE,sGAAsG,CAChHC,OAAO,CAAE,CACP,UAAU,CACV,WAAW,CACX,WAAW,CACX,WAAW,CACZ,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,6GACf,CAAC,CACD,CACEH,QAAQ,CAAE,yFAAyF,CACnGC,OAAO,CAAE,CACP,yBAAyB,CACzB,yBAAyB,CACzB,oDAAoD,CACpD,uCAAuC,CACxC,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,iIACf,CAAC,CACD,CACEH,QAAQ,CAAE,0EAA0E,CACpFC,OAAO,CAAE,CACP,OAAO,CACP,UAAU,CACV,WAAW,CACX,mBAAmB,CACpB,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,0HACf,CAAC,CACD,CACEH,QAAQ,CAAE,+EAA+E,CACzFC,OAAO,CAAE,CACP,0CAA0C,CAC1C,qCAAqC,CACrC,kFAAkF,CAClF,kCAAkC,CACnC,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,oKACf,CAAC,CACD,CACEH,QAAQ,CAAE,kEAAkE,CAC5EC,OAAO,CAAE,CACP,0BAA0B,CAC1B,iDAAiD,CACjD,kBAAkB,CAClB,0BAA0B,CAC3B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,uLACf,CAAC,CACD,CACEH,QAAQ,CAAE,0GAA0G,CACpHC,OAAO,CAAE,CACP,iCAAiC,CACjC,oCAAoC,CACpC,qEAAqE,CACrE,gDAAgD,CACjD,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,6LACf,CAAC,CACF,CACD,oCAAoC,CAAE,CACpC,CACEH,QAAQ,CAAE,iEAAiE,CAC3EC,OAAO,CAAE,CACP,8BAA8B,CAC9B,iCAAiC,CACjC,8BAA8B,CAC9B,gCAAgC,CACjC,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,+NACf,CAAC,CACD,CACEH,QAAQ,CAAE,+EAA+E,CACzFC,OAAO,CAAE,CACP,wFAAwF,CACxF,wFAAwF,CACxF,wFAAwF,CACxF,wFAAwF,CACzF,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,iPACf,CAAC,CACD,CACEH,QAAQ,CAAE,yDAAyD,CACnEC,OAAO,CAAE,CACP,uCAAuC,CACvC,oGAAoG,CACpG,2CAA2C,CAC3C,iDAAiD,CAClD,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,sQACf,CAAC,CACD,CACEH,QAAQ,CAAE,iFAAiF,CAC3FC,OAAO,CAAE,CACP,4BAA4B,CAC5B,iEAAiE,CACjE,oCAAoC,CACpC,sDAAsD,CACvD,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,gMACf,CAAC,CACD,CACEH,QAAQ,CAAE,0HAA0H,CACpIC,OAAO,CAAE,CACP,sBAAsB,CACtB,wBAAwB,CACxB,8DAA8D,CAC9D,6BAA6B,CAC9B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,6OACf,CAAC,CACD,CACEH,QAAQ,CAAE,4DAA4D,CACtEC,OAAO,CAAE,CACP,yCAAyC,CACzC,uCAAuC,CACvC,uCAAuC,CACvC,yCAAyC,CAC1C,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,4LACf,CAAC,CACD,CACEH,QAAQ,CAAE,mFAAmF,CAC7FC,OAAO,CAAE,CACP,uBAAuB,CACvB,sBAAsB,CACtB,oDAAoD,CACpD,iBAAiB,CAClB,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,uLACf,CAAC,CACD,CACEH,QAAQ,CAAE,sFAAsF,CAChGC,OAAO,CAAE,CACP,mDAAmD,CACnD,4EAA4E,CAC5E,8CAA8C,CAC9C,gCAAgC,CACjC,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,oKACf,CAAC,CACD,CACEH,QAAQ,CAAE,iFAAiF,CAC3FC,OAAO,CAAE,CACP,sCAAsC,CACtC,sEAAsE,CACtE,wCAAwC,CACxC,4BAA4B,CAC7B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,uJACf,CAAC,CACD,CACEH,QAAQ,CAAE,mCAAmC,CAC7CC,OAAO,CAAE,CACP,gCAAgC,CAChC,sCAAsC,CACtC,4BAA4B,CAC5B,qBAAqB,CACtB,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,mHACf,CAAC,CACD,CACEH,QAAQ,CAAE,qCAAqC,CAC/CC,OAAO,CAAE,CACP,wBAAwB,CACxB,0CAA0C,CAC1C,6BAA6B,CAC7B,8BAA8B,CAC/B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,4KACf,CAAC,CACD,CACEH,QAAQ,CAAE,8CAA8C,CACxDC,OAAO,CAAE,CACP,qBAAqB,CACrB,8BAA8B,CAC9B,sFAAsF,CACtF,yBAAyB,CAC1B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,kPACf,CAAC,CACD,CACEH,QAAQ,CAAE,mDAAmD,CAC7DC,OAAO,CAAE,CACP,iBAAiB,CACjB,iBAAiB,CACjB,YAAY,CACZ,gBAAgB,CACjB,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,oJACf,CAAC,CACD,CACEH,QAAQ,CAAE,0DAA0D,CACpEC,OAAO,CAAE,CACP,gCAAgC,CAChC,qEAAqE,CACrE,wBAAwB,CACxB,iCAAiC,CAClC,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,uMACf,CAAC,CACD,CACEH,QAAQ,CAAE,2CAA2C,CACrDC,OAAO,CAAE,CACP,iCAAiC,CACjC,8FAA8F,CAC9F,oCAAoC,CACpC,8CAA8C,CAC/C,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,8MACf,CAAC,CACD,CACEH,QAAQ,CAAE,0CAA0C,CACpDC,OAAO,CAAE,CACP,8BAA8B,CAC9B,qFAAqF,CACrF,sBAAsB,CACtB,wBAAwB,CACzB,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,wNACf,CAAC,CACD,CACEH,QAAQ,CAAE,gGAAgG,CAC1GC,OAAO,CAAE,CACP,gCAAgC,CAChC,mEAAmE,CACnE,6BAA6B,CAC7B,8BAA8B,CAC/B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,sJACf,CAAC,CACD,CACEH,QAAQ,CAAE,kEAAkE,CAC5EC,OAAO,CAAE,CACP,8BAA8B,CAC9B,8FAA8F,CAC9F,kCAAkC,CAClC,wCAAwC,CACzC,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,8KACf,CAAC,CACD,CACEH,QAAQ,CAAE,qKAAqK,CAC/KC,OAAO,CAAE,CACP,aAAa,CACb,aAAa,CACb,cAAc,CACd,cAAc,CACf,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,0MACf,CAAC,CACD,CACEH,QAAQ,CAAE,uEAAuE,CACjFC,OAAO,CAAE,CACP,4CAA4C,CAC5C,oFAAoF,CACpF,4DAA4D,CAC5D,kDAAkD,CACnD,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,2SACf,CAAC,CACF,CACD,qDAAqD,CAAE,CACrD,CACEH,QAAQ,CAAE,2DAA2D,CACrEC,OAAO,CAAE,CACP,+BAA+B,CAC/B,kDAAkD,CAClD,+BAA+B,CAC/B,0BAA0B,CAC3B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,yLACf,CAAC,CACD,CACEH,QAAQ,CAAE,2DAA2D,CACrEC,OAAO,CAAE,CACP,kCAAkC,CAClC,sDAAsD,CACtD,wCAAwC,CACxC,8CAA8C,CAC/C,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,gSACf,CAAC,CACD,CACEH,QAAQ,CAAE,oGAAoG,CAC9GC,OAAO,CAAE,CACP,kBAAkB,CAClB,qBAAqB,CACrB,qBAAqB,CACrB,eAAe,CAChB,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,qNACf,CAAC,CACD,CACEH,QAAQ,CAAE,wDAAwD,CAClEC,OAAO,CAAE,CACP,8BAA8B,CAC9B,yBAAyB,CACzB,oBAAoB,CACpB,qBAAqB,CACtB,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,kLACf,CAAC,CACD,CACEH,QAAQ,CAAE,sFAAsF,CAChGC,OAAO,CAAE,CACP,0CAA0C,CAC1C,4BAA4B,CAC5B,uDAAuD,CACvD,uCAAuC,CACxC,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,4MACf,CAAC,CACD,CACEH,QAAQ,CAAE,yDAAyD,CACnEC,OAAO,CAAE,CACP,2CAA2C,CAC3C,iCAAiC,CACjC,qBAAqB,CACrB,sBAAsB,CACvB,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,oMACf,CAAC,CACD,CACEH,QAAQ,CAAE,gFAAgF,CAC1FC,OAAO,CAAE,CACP,8BAA8B,CAC9B,4EAA4E,CAC5E,qCAAqC,CACrC,uCAAuC,CACxC,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,iNACf,CAAC,CACD,CACEH,QAAQ,CAAE,oDAAoD,CAC9DC,OAAO,CAAE,CACP,mBAAmB,CACnB,gBAAgB,CAChB,2BAA2B,CAC3B,8BAA8B,CAC/B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,kMACf,CAAC,CACD,CACEH,QAAQ,CAAE,gEAAgE,CAC1EC,OAAO,CAAE,CACP,gBAAgB,CAChB,yBAAyB,CACzB,6CAA6C,CAC7C,kBAAkB,CACnB,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,uLACf,CAAC,CACD,CACEH,QAAQ,CAAE,2EAA2E,CACrFC,OAAO,CAAE,CACP,UAAU,CACV,qCAAqC,CACrC,kBAAkB,CAClB,0BAA0B,CAC3B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,2IACf,CAAC,CACD,CACEH,QAAQ,CAAE,yFAAyF,CACnGC,OAAO,CAAE,CACP,2CAA2C,CAC3C,yEAAyE,CACzE,0BAA0B,CAC1B,0BAA0B,CAC3B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,gPACf,CAAC,CACD,CACEH,QAAQ,CAAE,+DAA+D,CACzEC,OAAO,CAAE,CACP,gDAAgD,CAChD,6FAA6F,CAC7F,yBAAyB,CACzB,sCAAsC,CACvC,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,kMACf,CAAC,CACD,CACEH,QAAQ,CAAE,mEAAmE,CAC7EC,OAAO,CAAE,CACP,mBAAmB,CACnB,6DAA6D,CAC7D,2BAA2B,CAC3B,uBAAuB,CACxB,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,2LACf,CAAC,CACD,CACEH,QAAQ,CAAE,iDAAiD,CAC3DC,OAAO,CAAE,CACP,oBAAoB,CACpB,kCAAkC,CAClC,8BAA8B,CAC9B,iBAAiB,CAClB,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,6OACf,CAAC,CACD,CACEH,QAAQ,CAAE,4CAA4C,CACtDC,OAAO,CAAE,CACP,0BAA0B,CAC1B,yFAAyF,CACzF,4BAA4B,CAC5B,oBAAoB,CACrB,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,kPACf,CAAC,CACD,CACEH,QAAQ,CAAE,4CAA4C,CACtDC,OAAO,CAAE,CACP,sCAAsC,CACtC,0BAA0B,CAC1B,6CAA6C,CAC7C,6CAA6C,CAC9C,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,4NACf,CAAC,CACD,CACEH,QAAQ,CAAE,sDAAsD,CAChEC,OAAO,CAAE,CACP,qCAAqC,CACrC,qBAAqB,CACrB,sCAAsC,CACtC,yBAAyB,CAC1B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,+MACf,CAAC,CACD,CACEH,QAAQ,CAAE,+CAA+C,CACzDC,OAAO,CAAE,CACP,6BAA6B,CAC7B,kGAAkG,CAClG,yBAAyB,CACzB,8BAA8B,CAC/B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,2NACf,CAAC,CACD,CACEH,QAAQ,CAAE,sEAAsE,CAChFC,OAAO,CAAE,CACP,2BAA2B,CAC3B,8EAA8E,CAC9E,4BAA4B,CAC5B,yBAAyB,CAC1B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,uOACf,CAAC,CACD,CACEH,QAAQ,CAAE,mKAAmK,CAC7KC,OAAO,CAAE,CACP,+CAA+C,CAC/C,gFAAgF,CAChF,kBAAkB,CAClB,iCAAiC,CAClC,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,2MACf,CAAC,CACF,CACD,0BAA0B,CAAE,CAC1B,CACEH,QAAQ,CAAE,kDAAkD,CAC5DC,OAAO,CAAE,CACP,2CAA2C,CAC3C,6EAA6E,CAC7E,qCAAqC,CACrC,sCAAsC,CACvC,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,sMACf,CAAC,CACD,CACEH,QAAQ,CAAE,gFAAgF,CAC1FC,OAAO,CAAE,CACP,2BAA2B,CAC3B,qGAAqG,CACrG,kCAAkC,CAClC,gDAAgD,CACjD,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,wLACf,CAAC,CACD,CACEH,QAAQ,CAAE,wDAAwD,CAClEC,OAAO,CAAE,CACP,0BAA0B,CAC1B,wBAAwB,CACxB,mEAAmE,CACnE,wBAAwB,CACzB,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,uLACf,CAAC,CACD,CACEH,QAAQ,CAAE,qDAAqD,CAC/DC,OAAO,CAAE,CACP,kBAAkB,CAClB,qBAAqB,CACrB,wBAAwB,CACxB,kBAAkB,CACnB,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,iKACf,CAAC,CACD,CACEH,QAAQ,CAAE,gEAAgE,CAC1EC,OAAO,CAAE,CACP,2CAA2C,CAC3C,sEAAsE,CACtE,8BAA8B,CAC9B,+BAA+B,CAChC,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,0KACf,CAAC,CACD,CACEH,QAAQ,CAAE,+DAA+D,CACzEC,OAAO,CAAE,CACP,sBAAsB,CACtB,yDAAyD,CACzD,0BAA0B,CAC1B,4BAA4B,CAC7B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,iMACf,CAAC,CACD,CACEH,QAAQ,CAAE,uDAAuD,CACjEC,OAAO,CAAE,CACP,0BAA0B,CAC1B,0DAA0D,CAC1D,8BAA8B,CAC9B,yBAAyB,CAC1B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,kMACf,CAAC,CACD,CACEH,QAAQ,CAAE,8DAA8D,CACxEC,OAAO,CAAE,CACP,sBAAsB,CACtB,6DAA6D,CAC7D,0BAA0B,CAC1B,yBAAyB,CAC1B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,wKACf,CAAC,CACD,CACEH,QAAQ,CAAE,oDAAoD,CAC9DC,OAAO,CAAE,CACP,0BAA0B,CAC1B,kDAAkD,CAClD,kBAAkB,CAClB,yBAAyB,CAC1B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,qKACf,CAAC,CACD,CACEH,QAAQ,CAAE,iDAAiD,CAC3DC,OAAO,CAAE,CACP,0CAA0C,CAC1C,gFAAgF,CAChF,0CAA0C,CAC1C,2BAA2B,CAC5B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,kOACf,CAAC,CACD,CACEH,QAAQ,CAAE,oDAAoD,CAC9DC,OAAO,CAAE,CACP,oCAAoC,CACpC,4FAA4F,CAC5F,wBAAwB,CACxB,8BAA8B,CAC/B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,uNACf,CAAC,CACD,CACEH,QAAQ,CAAE,oDAAoD,CAC9DC,OAAO,CAAE,CACP,2BAA2B,CAC3B,iCAAiC,CACjC,mBAAmB,CACnB,mBAAmB,CACpB,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,wJACf,CAAC,CACD,CACEH,QAAQ,CAAE,iEAAiE,CAC3EC,OAAO,CAAE,CACP,mBAAmB,CACnB,wDAAwD,CACxD,2BAA2B,CAC3B,sBAAsB,CACvB,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,4LACf,CAAC,CACD,CACEH,QAAQ,CAAE,iDAAiD,CAC3DC,OAAO,CAAE,CACP,iCAAiC,CACjC,eAAe,CACf,8BAA8B,CAC9B,+BAA+B,CAChC,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,2KACf,CAAC,CACD,CACEH,QAAQ,CAAE,8BAA8B,CACxCC,OAAO,CAAE,CACP,8BAA8B,CAC9B,iCAAiC,CACjC,oBAAoB,CACpB,+BAA+B,CAChC,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,uLACf,CAAC,CACD,CACEH,QAAQ,CAAE,4CAA4C,CACtDC,OAAO,CAAE,CACP,2BAA2B,CAC3B,mGAAmG,CACnG,oCAAoC,CACpC,4BAA4B,CAC7B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,2MACf,CAAC,CACD,CACEH,QAAQ,CAAE,8DAA8D,CACxEC,OAAO,CAAE,CACP,iCAAiC,CACjC,gHAAgH,CAChH,2BAA2B,CAC3B,oBAAoB,CACrB,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,mNACf,CAAC,CACD,CACEH,QAAQ,CAAE,oEAAoE,CAC9EC,OAAO,CAAE,CACP,kBAAkB,CAClB,4EAA4E,CAC5E,qBAAqB,CACrB,6BAA6B,CAC9B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,oMACf,CAAC,CACD,CACEH,QAAQ,CAAE,iEAAiE,CAC3EC,OAAO,CAAE,CACP,sCAAsC,CACtC,sFAAsF,CACtF,qCAAqC,CACrC,uBAAuB,CACxB,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,yLACf,CAAC,CACD,CACEH,QAAQ,CAAE,gFAAgF,CAC1FC,OAAO,CAAE,CACP,6BAA6B,CAC7B,2GAA2G,CAC3G,kCAAkC,CAClC,+BAA+B,CAChC,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,6NACf,CAAC,CACF,CACC,0BAA0B,CAAE,CAC1B;AACA,CACEH,QAAQ,CAAE,8FAA8F,CACxGC,OAAO,CAAE,CAAC,KAAK,CAAE,KAAK,CAAE,MAAM,CAAE,KAAK,CAAC,CACtCC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,oGACf,CAAC,CACD,CACEH,QAAQ,CAAE,uEAAuE,CACjFC,OAAO,CAAE,CAAC,gBAAgB,CAAE,cAAc,CAAE,mBAAmB,CAAE,sBAAsB,CAAC,CACxFC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,8EACf,CAAC,CAED;AACA,CACEH,QAAQ,CAAE,kFAAkF,CAC5FC,OAAO,CAAE,CAAC,eAAe,CAAE,eAAe,CAAE,QAAQ,CAAE,cAAc,CAAC,CACrEC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,+FACf,CAAC,CAED;AACA,CACEH,QAAQ,CAAE,4FAA4F,CACtGC,OAAO,CAAE,CACP,0CAA0C,CAC1C,sCAAsC,CACtC,kCAAkC,CAClC,kCAAkC,CACnC,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,wEACf,CAAC,CACD,CACEH,QAAQ,CAAE,8DAA8D,CACxEC,OAAO,CAAE,CAAC,gBAAgB,CAAE,QAAQ,CAAE,WAAW,CAAE,QAAQ,CAAC,CAC5DC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,+EACf,CAAC,CAED;AACA,CACEH,QAAQ,CAAE,mDAAmD,CAC7DC,OAAO,CAAE,CAAC,KAAK,CAAE,SAAS,CAAE,0BAA0B,CAAE,kBAAkB,CAAC,CAC3EC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,yDACf,CAAC,CACD,CACEH,QAAQ,CAAE,4EAA4E,CACtFC,OAAO,CAAE,CACP,2BAA2B,CAC3B,2BAA2B,CAC3B,2BAA2B,CAC3B,0BAA0B,CAC3B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,gEACf,CAAC,CAED;AACA,CACEH,QAAQ,CAAE,uEAAuE,CACjFC,OAAO,CAAE,CACP,kBAAkB,CAClB,sCAAsC,CACtC,oBAAoB,CACpB,wBAAwB,CACzB,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,uFACf,CAAC,CACD,CACEH,QAAQ,CAAE,mEAAmE,CAC7EC,OAAO,CAAE,CAAC,GAAG,CAAE,IAAI,CAAE,IAAI,CAAE,KAAK,CAAC,CACjCC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,2DACf,CAAC,CAED;AACA,CACEH,QAAQ,CAAE,2DAA2D,CACrEC,OAAO,CAAE,CACP,8BAA8B,CAC9B,yCAAyC,CACzC,kCAAkC,CAClC,gCAAgC,CACjC,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,2EACf,CAAC,CAED;AACA,CACEH,QAAQ,CAAE,4DAA4D,CACtEC,OAAO,CAAE,CACP,sBAAsB,CACtB,4BAA4B,CAC5B,2BAA2B,CAC3B,6BAA6B,CAC9B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,0DACf,CAAC,CAED;AACA,CACEH,QAAQ,CAAE,oEAAoE,CAC9EC,OAAO,CAAE,CACP,cAAc,CACd,mBAAmB,CACnB,oBAAoB,CACpB,iBAAiB,CAClB,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,mDACf,CAAC,CACD,CACEH,QAAQ,CAAE,kCAAkC,CAC5CC,OAAO,CAAE,CACP,oCAAoC,CACpC,+CAA+C,CAC/C,uCAAuC,CACvC,2CAA2C,CAC5C,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,6EACf,CAAC,CAED;AACA,CACEH,QAAQ,CAAE,0DAA0D,CACpEC,OAAO,CAAE,CACP,2BAA2B,CAC3B,qBAAqB,CACrB,qBAAqB,CACrB,gBAAgB,CACjB,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,uDACf,CAAC,CACD,CACEH,QAAQ,CAAE,oEAAoE,CAC9EC,OAAO,CAAE,CAAC,IAAI,CAAE,IAAI,CAAE,QAAQ,CAAE,wBAAwB,CAAC,CACzDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,qDACf,CAAC,CAED;AACA,CACEH,QAAQ,CAAE,6DAA6D,CACvEC,OAAO,CAAE,CACP,6CAA6C,CAC7C,oBAAoB,CACpB,uBAAuB,CACvB,gCAAgC,CACjC,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,wFACf,CAAC,CACD,CACEH,QAAQ,CAAE,mEAAmE,CAC7EC,OAAO,CAAE,CAAC,KAAK,CAAE,OAAO,CAAE,QAAQ,CAAE,OAAO,CAAC,CAC5CC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,gEACf,CAAC,CAED;AACA,CACEH,QAAQ,CAAE,sDAAsD,CAChEC,OAAO,CAAE,CACP,uBAAuB,CACvB,gBAAgB,CAChB,2BAA2B,CAC3B,yBAAyB,CAC1B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,iDACf,CAAC,CACD,CACEH,QAAQ,CAAE,gDAAgD,CAC1DC,OAAO,CAAE,CACP,wBAAwB,CACxB,iBAAiB,CACjB,iBAAiB,CACjB,oBAAoB,CACrB,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,qEACf,CAAC,CAED;AACA,CACEH,QAAQ,CAAE,oDAAoD,CAC9DC,OAAO,CAAE,CACP,sBAAsB,CACtB,oBAAoB,CACpB,sBAAsB,CACtB,kBAAkB,CACnB,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,uDACf,CAAC,CAED;AACA,CACEH,QAAQ,CAAE,qDAAqD,CAC/DC,OAAO,CAAE,CAAC,GAAG,CAAE,GAAG,CAAE,SAAS,CAAE,WAAW,CAAC,CAC3CC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,wCACf,CAAC,CACD,CACEH,QAAQ,CAAE,sDAAsD,CAChEC,OAAO,CAAE,CAAC,QAAQ,CAAE,QAAQ,CAAE,QAAQ,CAAE,QAAQ,CAAC,CACjDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,2DACf,CAAC,CACF,CACD,2CAA2C,CAAE,CAC/C,CACAH,QAAQ,CAAE,8CAA8C,CACxDC,OAAO,CAAE,CACT,+CAA+C,CAC/C,oEAAoE,CACpE,qCAAqC,CACrC,oDAAoD,CACnD,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,+IACb,CAAC,CACD,CACAH,QAAQ,CAAE,sFAAsF,CAChGC,OAAO,CAAE,CACT,gDAAgD,CAChD,0DAA0D,CAC1D,kDAAkD,CAClD,iDAAiD,CAChD,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,0GACb,CAAC,CACD,CACAH,QAAQ,CAAE,sEAAsE,CAChFC,OAAO,CAAE,CACT,+CAA+C,CAC/C,+CAA+C,CAC/C,iDAAiD,CACjD,+BAA+B,CAC9B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,iJACb,CAAC,CACD,CACAH,QAAQ,CAAE,mGAAmG,CAC7GC,OAAO,CAAE,CACT,WAAW,CACX,YAAY,CACZ,8CAA8C,CAC9C,4BAA4B,CAC3B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,6IACb,CAAC,CACD,CACAH,QAAQ,CAAE,2EAA2E,CACrFC,OAAO,CAAE,CACT,yBAAyB,CACzB,iCAAiC,CACjC,yBAAyB,CACzB,gCAAgC,CAC/B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,gHACb,CAAC,CACA,CACD,wDAAwD,CAAG,CAC3D,CACAH,QAAQ,CAAE,6EAA6E,CACvFC,OAAO,CAAE,CACT,mDAAmD,CACnD,oEAAoE,CACpE,2CAA2C,CAC3C,4BAA4B,CAC3B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,wIACb,CAAC,CACD,CACAH,QAAQ,CAAE,6EAA6E,CACvFC,OAAO,CAAE,CACT,mDAAmD,CACnD,iFAAiF,CACjF,iCAAiC,CACjC,sCAAsC,CACrC,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,+JACb,CAAC,CACD,CACAH,QAAQ,CAAE,iGAAiG,CAC3GC,OAAO,CAAE,CACT,KAAK,CACL,cAAc,CACd,eAAe,CACf,aAAa,CACZ,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,oIACb,CAAC,CACD,CACAH,QAAQ,CAAE,kEAAkE,CAC5EC,OAAO,CAAE,CACT,qBAAqB,CACrB,oDAAoD,CACpD,2CAA2C,CAC3C,kCAAkC,CACjC,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,kGACb,CAAC,CACD,CACAH,QAAQ,CAAE,yDAAyD,CACnEC,OAAO,CAAE,CACT,+CAA+C,CAC/C,4CAA4C,CAC5C,0DAA0D,CAC1D,kDAAkD,CACjD,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,2HACb,CAAC,CACA,CACD,+CAA+C,CAAG,CAClD,CACAH,QAAQ,CAAE,qEAAqE,CAC/EC,OAAO,CAAE,CACT,wDAAwD,CACxD,mDAAmD,CACnD,iDAAiD,CACjD,0DAA0D,CACzD,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,4JACb,CAAC,CAED,CACAH,QAAQ,CAAE,0GAA0G,CACpHC,OAAO,CAAE,CACT,eAAe,CACf,sBAAsB,CACtB,KAAK,CACL,aAAa,CACZ,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,wIACb,CAAC,CAED,CACAH,QAAQ,CAAE,6DAA6D,CACvEC,OAAO,CAAE,CACT,4DAA4D,CAC5D,kCAAkC,CAClC,kDAAkD,CAClD,4BAA4B,CAC3B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,6JACb,CAAC,CAED,CACAH,QAAQ,CAAE,oHAAoH,CAC9HC,OAAO,CAAE,CACT,UAAU,CACV,QAAQ,CACR,UAAU,CACV,QAAQ,CACP,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,4HACb,CAAC,CAED,CACAH,QAAQ,CAAE,4DAA4D,CACtEC,OAAO,CAAE,CACT,+CAA+C,CAC/C,oDAAoD,CACpD,wCAAwC,CACxC,mBAAmB,CAClB,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,2IACb,CAAC,CAED,CACAH,QAAQ,CAAE,mHAAmH,CAC7HC,OAAO,CAAE,CACT,qBAAqB,CACrB,oBAAoB,CACpB,iBAAiB,CACjB,kBAAkB,CACjB,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,6HACb,CAAC,CAED,CACAH,QAAQ,CAAE,4EAA4E,CACtFC,OAAO,CAAE,CACT,sCAAsC,CACtC,qEAAqE,CACrE,0BAA0B,CAC1B,kCAAkC,CACjC,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,kKACb,CAAC,CAED,CACAH,QAAQ,CAAE,oEAAoE,CAC9EC,OAAO,CAAE,CACT,oEAAoE,CACpE,qCAAqC,CACrC,4BAA4B,CAC5B,2BAA2B,CAC1B,CACDC,OAAO,CAAE,CAAC,CACVC,WAAW,CAAE,iHACb,CAAC,CAGD,CAAC,CAGD;AACA,KAAM,CAAAC,cAAc,CAAG,CACvB,CAAC,CAED;AACA,KAAM,CAAAC,cAAc,CAAG,CACrB,yBAAyB,CAAE,gBAAgB,CAC3C,yCAAyC,CAAE,WAAW,CACtD,0DAA0D,CAAE,WAAW,CACvE,oCAAoC,CAAE,WAAW,CACjD,qDAAqD,CAAE,WAAW,CAClE,0CAA0C,CAAE,WAAW,CACvD,uCAAuC,CAAE,WAAW,CACpD,2CAA2C,CAAE,WAAW,CACxD,wDAAwD,CAAE,WAAW,CACrE,+CAA+C,CAAE,YACnD,CAAC,CAED;AACA,KAAM,CAAAC,qBAAqB,CAAG,CAAC,CAAC,CAChCC,MAAM,CAACC,IAAI,CAACH,cAAc,CAAC,CAACI,OAAO,CAACC,WAAW,EAAI,CACjDJ,qBAAqB,CAACD,cAAc,CAACK,WAAW,CAAC,CAAC,CAAGA,WAAW,CAClE,CAAC,CAAC,CAEF,OAASX,QAAQ,CAAEK,cAAc,CAAEC,cAAc,CAAEC,qBAAqB","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}